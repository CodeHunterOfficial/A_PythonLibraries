{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4lD5M749A218m/OikhNks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/A_PythonLibraries/blob/main/%D0%91%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0_Scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Библиотека Scikit-learn\n",
        "\n",
        "Библиотека **Scikit-learn** является одной из самых популярных и мощных библиотек для машинного обучения в Python. Она предоставляет множество инструментов для анализа данных и реализации различных алгоритмов машинного обучения (как для задач классификации, так и для регрессии, кластеризации, снижения размерности и т.д.). В этой лекции мы подробно рассмотрим основные концепции библиотеки, ее функционал и примеры использования.\n",
        "\n",
        "#### 1. Введение в Scikit-learn\n",
        "\n",
        "**Scikit-learn** построена на базе других библиотек, таких как NumPy, SciPy и Matplotlib, что делает ее интегрированной частью экосистемы научных вычислений на Python. Основные особенности библиотеки:\n",
        "- Простота в использовании и доступная документация.\n",
        "- Широкий выбор алгоритмов машинного обучения.\n",
        "- Поддержка предобработки данных.\n",
        "- Возможность оценки модели с помощью различных метрик.\n",
        "- Возможности кросс-валидации и подбора гиперпараметров.\n",
        "\n",
        "#### 2. Структура Scikit-learn\n",
        "\n",
        "Все объекты и функционал библиотеки можно разделить на несколько категорий:\n",
        "- **Алгоритмы машинного обучения**: классификация, регрессия, кластеризация, снижение размерности.\n",
        "- **Предобработка данных**: масштабирование, нормализация, кодирование категориальных признаков и т.д.\n",
        "- **Оценка модели**: кросс-валидация, метрики точности, подбор гиперпараметров.\n",
        "- **Модельный пайплайн**: позволяет объединять несколько шагов обработки и обучения в одном процессе.\n",
        "\n",
        "#### 3. Классификация и регрессия\n",
        "\n",
        "##### 3.1. Классификация\n",
        "Классификация — это задача, при которой модель пытается отнести объект к одному из заранее определенных классов.\n",
        "\n",
        "**Пример 1: Классификация с использованием метода K ближайших соседей (KNN)**\n",
        "\n",
        "Метод **K ближайших соседей** (K-Nearest Neighbors, KNN) — один из простых алгоритмов классификации, который присваивает класс объекту на основе ближайших соседей в пространстве признаков. В этом примере мы будем использовать набор данных \"Ирисы\", который состоит из данных о трех классах ирисов.\n",
        "\n",
        "Перед тем, как использовать алгоритм KNN, сначала нужно импортировать необходимые библиотеки и загрузить набор данных. Затем мы разделим данные на обучающую и тестовую выборки, чтобы модель могла учиться на одном подмножестве данных и проверять свои результаты на другом. После этого мы создадим и обучим модель KNN, а затем оценим ее точность.\n",
        "\n",
        "```python\n",
        "# Импортируем необходимые библиотеки\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Загружаем набор данных \"Ирисы\"\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Разделяем данные на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Создаем модель KNN с числом соседей 3\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Обучаем модель\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Делаем предсказания\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Оцениваем точность\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Точность классификации: {accuracy}\")\n",
        "```\n",
        "\n",
        "**Объяснение кода**:\n",
        "1. Мы импортируем необходимые модули из библиотеки Scikit-learn.\n",
        "2. Загружаем набор данных \"Ирисы\" и извлекаем из него признаки (X) и метки классов (y).\n",
        "3. С помощью функции `train_test_split` разделяем данные на обучающую (70%) и тестовую (30%) выборки.\n",
        "4. Создаем экземпляр класса `KNeighborsClassifier`, указывая количество ближайших соседей (3 в данном случае).\n",
        "5. Обучаем модель с помощью метода `fit`, передавая обучающие данные.\n",
        "6. Используем метод `predict`, чтобы получить предсказания для тестовой выборки.\n",
        "7. Оцениваем точность модели с помощью функции `accuracy_score` и выводим результат.\n",
        "\n",
        "##### 3.2. Регрессия\n",
        "Регрессия используется, когда нужно предсказать числовое значение на основе данных. В этом примере мы используем линейную регрессию для предсказания цен на дома.\n",
        "\n",
        "**Пример 2: Линейная регрессия**\n",
        "\n",
        "Линейная регрессия — это алгоритм, который строит линейную зависимость между независимыми переменными (фичами) и зависимой переменной (целевым значением). Мы будем использовать набор данных, который содержит информацию о ценах на дома в Бостоне.\n",
        "\n",
        "Перед тем, как обучить модель линейной регрессии, нам нужно загрузить данные, разделить их на обучающую и тестовую выборки, а затем создать и обучить модель. После обучения мы сделаем предсказания на тестовой выборке и оценим качество модели с помощью средней квадратичной ошибки.\n",
        "\n",
        "```python\n",
        "# Импортируем необходимые библиотеки\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Загружаем набор данных по ценам на дома в Бостоне\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Разделяем данные на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Создаем модель линейной регрессии\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Обучаем модель\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Делаем предсказания\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# Оцениваем модель с помощью метрики средней квадратичной ошибки\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Средняя квадратичная ошибка: {mse}\")\n",
        "```\n",
        "\n",
        "**Объяснение кода**:\n",
        "1. Импортируем необходимые библиотеки для работы с линейной регрессией и метриками.\n",
        "2. Загружаем набор данных по ценам на дома в Бостоне и извлекаем признаки (X) и целевые значения (y).\n",
        "3. Делим данные на обучающую и тестовую выборки с помощью функции `train_test_split`.\n",
        "4. Создаем экземпляр класса `LinearRegression` для модели линейной регрессии.\n",
        "5. Обучаем модель на тренировочных данных с помощью метода `fit`.\n",
        "6. Предсказываем значения для тестовой выборки с помощью метода `predict`.\n",
        "7. Оцениваем модель по средней квадратичной ошибке (MSE) с помощью функции `mean_squared_error`.\n",
        "\n",
        "#### 4. Предобработка данных\n",
        "\n",
        "Для большинства моделей машинного обучения важно правильно предобработать данные. Scikit-learn предлагает множество инструментов для этого.\n",
        "\n",
        "##### 4.1. Масштабирование признаков\n",
        "Многие алгоритмы машинного обучения чувствительны к масштабам признаков (например, логистическая регрессия или метод опорных векторов). Масштабирование помогает сделать так, чтобы все признаки имели одинаковый масштаб. Это особенно важно, если признаки имеют разные единицы измерения.\n",
        "\n",
        "**Пример 3: Стандартизация данных**\n",
        "\n",
        "В этом примере мы используем `StandardScaler` для масштабирования признаков. Он стандартизирует данные так, чтобы они имели среднее 0 и стандартное отклонение 1.\n",
        "\n",
        "```python\n",
        "# Импортируем StandardScaler для масштабирования\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Создаем набор данных (2 признака)\n",
        "X = np.array([[1, 2], [2, 3], [4, 5], [6, 7]])\n",
        "\n",
        "# Создаем объект StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Обучаем и трансформируем данные\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Исходные данные:\\n\", X)\n",
        "print(\"Масштабированные данные:\\n\", X_scaled)\n",
        "```\n",
        "\n",
        "**Объяснение кода**:\n",
        "1. Импортируем класс `StandardScaler` для масштабирования и библиотеку NumPy.\n",
        "2. Создаем массив NumPy, представляющий набор данных с двумя признаками.\n",
        "3. Создаем экземпляр `StandardScaler`, который будет использоваться для стандартизации данных.\n",
        "4. Применяем метод `fit_transform` для обучения на исходных данных и трансформации их в масштабированные данные.\n",
        "5. Выводим исходные и масштабированные данные для сравнения.\n",
        "\n",
        "##### 4.2. Кодирование категориальных признаков\n",
        "Категориальные признаки должны быть преобразованы в числовые перед тем, как их можно будет использовать в моделях машинного обучения. Один из распространенных способов — использование one-hot кодирования, при котором каждый уникальный класс представляется отдельным бинарным признаком.\n",
        "\n",
        "**Пример 4: One-hot кодирование**\n",
        "\n",
        "В этом примере мы используем `OneHotEncoder`, чтобы закодировать категориальные данные в двоичные вектора.\n",
        "\n",
        "```python\n",
        "# Импортируем OneHotEncoder\n",
        "from\n",
        "\n",
        " sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Создаем категориальные данные\n",
        "X = np.array([['Male'], ['Female'], ['Female'], ['Male']])\n",
        "\n",
        "# Создаем объект OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Трансформируем данные\n",
        "X_encoded = encoder.fit_transform(X).toarray()\n",
        "\n",
        "print(\"Закодированные данные:\\n\", X_encoded)\n",
        "```\n",
        "\n",
        "**Объяснение кода**:\n",
        "1. Импортируем класс `OneHotEncoder` из модуля `preprocessing`.\n",
        "2. Создаем массив NumPy с категориальными данными (пол).\n",
        "3. Создаем экземпляр `OneHotEncoder`, который будет использоваться для кодирования.\n",
        "4. Применяем метод `fit_transform`, чтобы закодировать данные и преобразовать их в массив NumPy.\n",
        "5. Выводим закодированные данные для просмотра.\n",
        "\n",
        "#### 5. Оценка качества модели\n",
        "\n",
        "После того, как модель обучена, необходимо оценить, насколько хорошо она работает. В Scikit-learn существует множество метрик для этого.\n",
        "\n",
        "##### 5.1. Метрики для классификации\n",
        "Одной из самых распространенных метрик для классификации является точность (accuracy), которая измеряет долю правильно классифицированных объектов. Также можно использовать метрики, такие как F1-скор и полнота, для более глубокого анализа.\n",
        "\n",
        "**Пример 5: Оценка модели классификации**\n",
        "\n",
        "В этом примере мы будем использовать функцию `classification_report`, чтобы получить подробный отчет по метрикам классификации для модели KNN, которую мы обучали ранее.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Оцениваем модель по метрикам precision, recall, f1-score\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "```\n",
        "\n",
        "**Объяснение кода**:\n",
        "1. Импортируем функцию `classification_report` из модуля `metrics`.\n",
        "2. Используем `classification_report`, чтобы получить метрики для тестовой выборки, включая точность (precision), полноту (recall) и F1-скор.\n",
        "3. Выводим отчет, который включает метрики для каждого класса в наборе данных \"Ирисы\".\n",
        "\n",
        "##### 5.2. Метрики для регрессии\n",
        "Для регрессии часто используются такие метрики, как **среднеквадратичная ошибка (MSE)** и **средняя абсолютная ошибка (MAE)**. Эти метрики позволяют оценить, насколько близки предсказанные значения к истинным.\n",
        "\n",
        "**Пример 6: Оценка модели регрессии**\n",
        "\n",
        "Мы будем использовать метрику средней абсолютной ошибки для оценки качества линейной регрессии.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Оцениваем модель с помощью средней абсолютной ошибки\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Средняя абсолютная ошибка: {mae}\")\n",
        "```\n",
        "\n",
        "**Объяснение кода**:\n",
        "1. Импортируем функцию `mean_absolute_error` из модуля `metrics`.\n",
        "2. Вычисляем среднюю абсолютную ошибку, используя тестовые данные и предсказанные значения.\n",
        "3. Выводим значение средней абсолютной ошибки, что позволяет оценить качество модели.\n",
        "\n",
        "#### 6. Кросс-валидация и подбор гиперпараметров\n",
        "\n",
        "##### 6.1. Кросс-валидация\n",
        "\n",
        "Кросс-валидация помогает избежать переобучения путем использования различных подмножеств данных для обучения и тестирования. Один из распространенных методов — **K-fold** кросс-валидация, где данные разбиваются на K частей, и модель обучается K раз, каждый раз используя одну часть для тестирования, а остальные для обучения.\n",
        "\n",
        "**Пример 7: K-fold кросс-валидация**\n",
        "\n",
        "В этом примере мы будем использовать `cross_val_score`, чтобы оценить модель KNN с помощью 5-fold кросс-валидации.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Оцениваем модель с помощью 5-fold кросс-валидации\n",
        "scores = cross_val_score(knn, X, y, cv=5)\n",
        "\n",
        "print(\"Точности для каждой из 5 складок:\", scores)\n",
        "print(\"Средняя точность:\", scores.mean())\n",
        "```\n",
        "\n",
        "**Объяснение кода**:\n",
        "1. Импортируем функцию `cross_val_score` из модуля `model_selection`.\n",
        "2. Используем `cross_val_score`, чтобы оценить модель KNN с 5-fold кросс-валидацией, передавая данные и целевые значения.\n",
        "3. Выводим точности для каждой из 5 фолдов и среднюю точность по всем фолдам.\n",
        "\n",
        "##### 6.2. Подбор гиперпараметров\n",
        "\n",
        "Подбор гиперпараметров помогает найти оптимальные параметры для модели. Один из распространенных методов — **Grid Search**. С его помощью можно проверить различные комбинации параметров и выбрать наилучшие.\n",
        "\n",
        "**Пример 8: Grid Search для подбора гиперпараметров**\n",
        "\n",
        "В этом примере мы будем использовать `GridSearchCV`, чтобы найти наилучшие параметры для модели KNN.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Определяем параметры для подбора\n",
        "param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
        "\n",
        "# Создаем объект GridSearchCV\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "\n",
        "# Обучаем модель с помощью Grid Search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Лучшие параметры\n",
        "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
        "```\n",
        "\n",
        "**Объяснение кода**:\n",
        "1. Импортируем класс `GridSearchCV` из модуля `model_selection`.\n",
        "2. Определяем словарь `param_grid`, в котором указываем параметры для подбора — в данном случае количество соседей.\n",
        "3. Создаем экземпляр `GridSearchCV`, передавая модель KNN и параметры для подбора, устанавливая кросс-валидацию на 5 фолдов.\n",
        "4. Обучаем модель с помощью метода `fit`, чтобы выполнить поиск по сетке.\n",
        "5. Выводим лучшие найденные параметры, что позволяет понять, какие настройки дают наилучшие результаты.\n",
        "\n",
        "#### Заключение\n",
        "\n",
        "Scikit-learn — это мощный инструмент для реализации алгоритмов машинного обучения и предобработки данных. Он предлагает разнообразные алгоритмы, инструменты для оценки моделей и предобработки данных, а также удобные средства для работы с гиперпараметрами. В этой лекции мы рассмотрели основные аспекты работы с библиотекой, но возможности Scikit-learn намного шире, и рекомендуется изучать документацию для более глубокого понимания."
      ],
      "metadata": {
        "id": "mxd-FO9mzh-0"
      }
    }
  ]
}