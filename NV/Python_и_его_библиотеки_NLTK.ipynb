{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/A_PythonLibraries/blob/main/NV/Python_%D0%B8_%D0%B5%D0%B3%D0%BE_%D0%B1%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B8_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Библиотека NLTK\n",
        "\n",
        "## Введение в NLTK\n",
        "\n",
        "**NLTK (Natural Language Toolkit)** — это мощная библиотека Python для обработки естественного языка (NLP). Она предоставляет множество инструментов и ресурсов для работы с текстом, включая токенизацию, стемминг, лемматизацию, разбор синтаксиса и многие другие задачи. Библиотека особенно полезна для студентов, исследователей и разработчиков, работающих в области обработки текста и создания языковых моделей.\n",
        "\n",
        "### Установка NLTK\n",
        "\n",
        "Перед тем как приступить к практическим примерам, необходимо установить библиотеку NLTK. Для этого выполните следующую команду в терминале или командной строке:"
      ],
      "metadata": {
        "id": "_A6cwktII_eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spmFmHUjKRlH",
        "outputId": "7ecdf305-2c11-42e8-a2f8-d63f0ef80940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "После установки мы можем импортировать библиотеку и загрузить необходимые ресурсы.\n",
        "\n",
        "### Импортирование NLTK и загрузка ресурсов\n",
        "\n",
        "NLTK предоставляет множество корпусов и ресурсов, которые можно загрузить. Для начала мы импортируем библиотеку и загрузим набор данных."
      ],
      "metadata": {
        "id": "MTFLk5DcI_jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MDyWpAHrI_ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Для токенизации\n",
        "nltk.download('wordnet')  # Для лемматизации\n",
        "nltk.download('stopwords')  # Для работы со стоп-словами"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGzRRp0-KYrS",
        "outputId": "21359664-76ec-4afe-fe23-26fd7f53741c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Токенизация\n",
        "\n",
        "Токенизация — это процесс разделения текста на составные части, такие как слова или предложения. В NLTK это делается с помощью функции `nltk.word_tokenize()` для слов и `nltk.sent_tokenize()` для предложений.\n",
        "\n",
        "### Пример токенизации\n",
        "\n",
        "#### На русском"
      ],
      "metadata": {
        "id": "QBxqPzXfI_od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Привет, как дела? Сегодня прекрасный день!\"\n",
        "sentences = sent_tokenize(text, language='russian')\n",
        "words = word_tokenize(text, language='russian')\n",
        "\n",
        "print(\"Предложения:\", sentences)\n",
        "print(\"Слова:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBXqzq-aKYfs",
        "outputId": "1722ff5e-990a-438a-db6a-4a131741bee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предложения: ['Привет, как дела?', 'Сегодня прекрасный день!']\n",
            "Слова: ['Привет', ',', 'как', 'дела', '?', 'Сегодня', 'прекрасный', 'день', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** В данном примере мы разделяем текст на предложения и слова. Функция `sent_tokenize()` разбивает текст на предложения, а `word_tokenize()` — на отдельные слова.\n",
        "\n",
        "#### In English"
      ],
      "metadata": {
        "id": "lrgVXyvwI_ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello, how are you? It's a beautiful day today!\"\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbWlhvPgKYTG",
        "outputId": "07d54190-a00c-40bd-8fe2-88cdef5c4f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Hello, how are you?', \"It's a beautiful day today!\"]\n",
            "Words: ['Hello', ',', 'how', 'are', 'you', '?', 'It', \"'s\", 'a', 'beautiful', 'day', 'today', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** Here, we split the text into sentences and words using the same two functions. The output will show how the text is broken down.\n",
        "\n",
        "## Стоп-слова\n",
        "\n",
        "Стоп-слова — это слова, которые часто встречаются в тексте, но не несут значительной смысловой нагрузки (например, \"и\", \"в\", \"на\"). В NLTK можно получить список стоп-слов для различных языков.\n",
        "\n",
        "### Пример работы со стоп-словами\n",
        "\n",
        "#### На русском"
      ],
      "metadata": {
        "id": "ZR-alHEDI_tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "text = \"Это простой пример текста с некоторыми стоп-словами.\"\n",
        "words = word_tokenize(text, language='russian')\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Исходные слова:\", words)\n",
        "print(\"Фильтрованные слова:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJFZScxfKYBk",
        "outputId": "0c8a5a3f-78eb-419b-e085-6d08e3a1b5f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходные слова: ['Это', 'простой', 'пример', 'текста', 'с', 'некоторыми', 'стоп-словами', '.']\n",
            "Фильтрованные слова: ['Это', 'простой', 'пример', 'текста', 'некоторыми', 'стоп-словами', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы загружаем список стоп-слов на русском языке и используем его для фильтрации слов из нашего текста, оставляя только те, которые имеют смысл.\n",
        "\n",
        "#### In English"
      ],
      "metadata": {
        "id": "yIBOC7lSI_vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "text = \"This is a simple example of a text with some stop words.\"\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPDOqnDmKX1B",
        "outputId": "9e7bc89b-8606-4205-aad3-c997f4fefbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['This', 'is', 'a', 'simple', 'example', 'of', 'a', 'text', 'with', 'some', 'stop', 'words', '.']\n",
            "Filtered words: ['simple', 'example', 'text', 'stop', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** In this example, we load the list of stop words in English and filter them out from the text, keeping only meaningful words.\n",
        "\n",
        "## Стемминг и Лемматизация\n",
        "\n",
        "### Стемминг\n",
        "\n",
        "Стемминг — это процесс уменьшения слов до их корня, что может быть полезно для анализа текстов. В NLTK есть несколько стеммеров, таких как `PorterStemmer`.\n",
        "\n",
        "#### Пример стемминга\n",
        "\n",
        "#### На русском"
      ],
      "metadata": {
        "id": "Gq1cz4xWI_yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "words = [\"бегать\", \"бегал\", \"бегущий\", \"бегу\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Стеммированные слова:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOJR4LLWKXnh",
        "outputId": "72ccdf54-29e6-4591-d692-0228371858c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стеммированные слова: ['бега', 'бега', 'бегущ', 'бег']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы используем стеммер для русского языка, чтобы получить корни различных форм слова \"бегать\".\n",
        "\n",
        "#### In English"
      ],
      "metadata": {
        "id": "Y6BofAlBI_1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"ran\", \"runner\", \"runs\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jX7z6QCKXea",
        "outputId": "2aee69e1-3068-42b5-abf4-c7e9120539ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['run', 'ran', 'runner', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** Here, we use the Porter stemmer to reduce different forms of the word \"run\" to their root.\n",
        "\n",
        "### Лемматизация\n",
        "\n",
        "Лемматизация — более сложный процесс, который приводит слово к его начальной форме (лемме). В NLTK это можно сделать с помощью `WordNetLemmatizer`.\n",
        "\n",
        "#### Пример лемматизации\n",
        "\n",
        "#### На русском"
      ],
      "metadata": {
        "id": "AXHk_bGhI_3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"бегал\", \"бегущий\", \"бегу\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(\"Лемматизированные слова:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b5KrOHOKXSd",
        "outputId": "b33258da-e0c5-4213-97a6-28d4bdd02581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные слова: ['бегал', 'бегущий', 'бегу']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы используем лемматизатор для получения начальной формы слова. Здесь мы указываем, что слова — это глаголы.\n",
        "\n",
        "#### In English"
      ],
      "metadata": {
        "id": "Q-PsFsktI_8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"ran\", \"runner\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYEM9sY9KXC9",
        "outputId": "a8f90533-20b2-4005-ee07-eb4d19841045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['run', 'run', 'runner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** In this example, we use the lemmatizer to convert different forms of the verb \"run\" into its base form.\n",
        "\n",
        "## Часть речи (POS) Тегирование\n",
        "\n",
        "Тегирование частей речи — это процесс обозначения слов в тексте их соответствующими частями речи (существительное, глагол и т. д.). NLTK предоставляет инструменты для этого.\n",
        "\n",
        "### Пример POS-тегирования\n",
        "\n",
        "#### На русском"
      ],
      "metadata": {
        "id": "f11ZRwDXI_-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('cess_esp')\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"Сегодня солнечный день.\"\n",
        "words = word_tokenize(text, language='russian')\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "print(\"Тегированные слова:\", tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nsz00nFKW0y",
        "outputId": "3898ae5e-5ac6-4821-caf1-0215eeaaee9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тегированные слова: [('Сегодня', 'JJ'), ('солнечный', 'NNP'), ('день', 'NNP'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы используем токенизацию для получения списка слов, а затем применяем тегирование частей речи для их обозначения.\n",
        "\n",
        "#### In English"
      ],
      "metadata": {
        "id": "PyYhkkALK9cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"Today is a sunny day.\"\n",
        "words = word_tokenize(text)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "print(\"Tagged words:\", tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_mzegpYK9r0",
        "outputId": "bf775b70-3e0b-416f-c385-b17adeadab61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged words: [('Today', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('sunny', 'JJ'), ('day', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** We tokenize the text and then tag the words with their corresponding parts of speech.\n",
        "\n",
        "Библиотека NLTK (Natural Language Toolkit) обладает множеством функций, выходящих далеко за рамки базовой обработки текста, такой как токенизация, стемминг или работа со стоп-словами. Рассмотрим дополнительные возможности NLTK, которые также могут быть полезны при решении задач обработки естественного языка.\n",
        "\n",
        "### 1. **Работа с корпусами и текстовыми коллекциями**\n",
        "\n",
        "NLTK содержит большой набор **корпусов** (коллекций текстов), которые можно использовать для обучения моделей, тестирования алгоритмов и экспериментов с обработкой текстов. Корпуса NLTK включают в себя книги, статьи, аннотации и даже примеры диалогов.\n",
        "\n",
        "#### Пример работы с корпусом"
      ],
      "metadata": {
        "id": "ENAPpgyoK94X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Загрузка текста \"Гамлет\" из корпуса\n",
        "hamlet_text = gutenberg.raw('shakespeare-hamlet.txt')\n",
        "\n",
        "# Вывод первых 500 символов\n",
        "print(hamlet_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrkmBar1K-J3",
        "outputId": "6f38404c-4653-497f-cf66-c75ca0da4510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[The Tragedie of Hamlet by William Shakespeare 1599]\n",
            "\n",
            "\n",
            "Actus Primus. Scoena Prima.\n",
            "\n",
            "Enter Barnardo and Francisco two Centinels.\n",
            "\n",
            "  Barnardo. Who's there?\n",
            "  Fran. Nay answer me: Stand & vnfold\n",
            "your selfe\n",
            "\n",
            "   Bar. Long liue the King\n",
            "\n",
            "   Fran. Barnardo?\n",
            "  Bar. He\n",
            "\n",
            "   Fran. You come most carefully vpon your houre\n",
            "\n",
            "   Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
            "\n",
            "   Fran. For this releefe much thankes: 'Tis bitter cold,\n",
            "And I am sicke at heart\n",
            "\n",
            "   Barn. Haue you had quiet Guard?\n",
            "  Fran. Not\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** В этом примере мы используем корпус **Gutenberg**, чтобы получить текст \"Гамлет\" Уильяма Шекспира. Корпус NLTK содержит много классических текстов и полезен для анализа литературы или других текстов.\n",
        "\n",
        "### 2. **N-граммы и моделирование текста**\n",
        "\n",
        "**N-граммы** — это последовательности из N слов, которые используются для моделирования языка, анализа текста и построения языковых моделей. NLTK позволяет легко генерировать и анализировать такие последовательности.\n",
        "\n",
        "#### Пример создания N-грамм"
      ],
      "metadata": {
        "id": "aipwH60RK-XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sentence = \"Программирование на Python с использованием NLTK\"\n",
        "words = word_tokenize(sentence, language='russian')\n",
        "\n",
        "# Генерация биграмм (2-грамм)\n",
        "bigrams = list(ngrams(words, 2))\n",
        "\n",
        "print(\"Биграммы:\", bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6BfreRfK-k-",
        "outputId": "e4f5448b-59d1-48c1-a1e2-a622a2475ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Биграммы: [('Программирование', 'на'), ('на', 'Python'), ('Python', 'с'), ('с', 'использованием'), ('использованием', 'NLTK')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** В данном примере мы создаём биграммы — последовательности из двух слов. N-граммы полезны для задач предсказания текста, анализа коллокаций (часто встречающихся пар слов) и машинного перевода.\n",
        "\n",
        "### 3. **Анализ коллокаций**\n",
        "\n",
        "Коллокации — это часто встречающиеся сочетания слов, такие как \"сильный дождь\" или \"зеленый свет\". NLTK предоставляет механизмы для поиска таких комбинаций в тексте.\n",
        "\n",
        "#### Пример анализа коллокаций"
      ],
      "metadata": {
        "id": "iINcATu4K-vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "\n",
        "text = word_tokenize(\"NLTK is a leading platform for building Python programs to work with human language data.\", language='english')\n",
        "\n",
        "# Найдём часто встречающиеся биграммы\n",
        "finder = BigramCollocationFinder.from_words(text)\n",
        "collocations = finder.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n",
        "\n",
        "print(\"Коллокации:\", collocations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etk44_scK--m",
        "outputId": "f9d72765-e589-4db6-9286-c9c225202d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Коллокации: [('NLTK', 'is'), ('Python', 'programs'), ('a', 'leading'), ('building', 'Python'), ('data', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы используем `BigramCollocationFinder`, чтобы найти наиболее частотные биграммы в тексте. Анализ коллокаций полезен для улучшения качества текста и работы с лексическими моделями.\n",
        "\n",
        "### 4. **Классификация текста**\n",
        "\n",
        "NLTK содержит инструменты для классификации текста на основе машинного обучения. Это полезно для задач, таких как классификация тональности (например, положительный или отрицательный отзыв), классификация новостей и т.д. NLTK предоставляет несколько классификаторов, включая наивный байесовский классификатор.\n",
        "\n",
        "#### Пример классификации текста"
      ],
      "metadata": {
        "id": "ElZavWK-K_Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk import NaiveBayesClassifier\n",
        "# from nltk.corpus import movie_reviews\n",
        "# import random\n",
        "\n",
        "# nltk.download('movie_reviews')\n",
        "\n",
        "# # Подготовка данных\n",
        "# documents = [(list(movie_reviews.words(fileid)), category)\n",
        "#              for category in movie_reviews.categories()\n",
        "#              for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# random.shuffle(documents)\n",
        "\n",
        "# # Извлечение признаков\n",
        "# def document_features(document):\n",
        "#     words = set(document)\n",
        "#     features = {}\n",
        "#     for word in movie_reviews.words():\n",
        "#         features[word] = (word in words)\n",
        "#     return features\n",
        "\n",
        "# # Подготовка тренировочных данных\n",
        "# featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "# train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "\n",
        "# # Обучение наивного байесовского классификатора\n",
        "# classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# # Тестирование классификатора\n",
        "# accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "# print(f\"Точность классификации: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "g2IZXu0-K_c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы используем **наивный байесовский классификатор**, чтобы классифицировать отзывы о фильмах как положительные или отрицательные. NLTK позволяет легко строить модели классификации для различных задач анализа текста.\n",
        "\n",
        "### 5. **Синтаксический анализ (Parsing)**\n",
        "\n",
        "Синтаксический анализ (или разбор) используется для построения синтаксических деревьев, которые показывают структуру предложения. Это важно для задач глубинного анализа текста, таких как извлечение сущностей и отношение между частями предложения.\n",
        "\n",
        "#### Пример синтаксического анализа"
      ],
      "metadata": {
        "id": "9luhGGIAK_oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import CFG\n",
        "from nltk import ChartParser\n",
        "\n",
        "# Определяем контекстно-свободную грамматику\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  NP -> Det N\n",
        "  VP -> V NP\n",
        "  Det -> 'the'\n",
        "  N -> 'cat' | 'dog'\n",
        "  V -> 'chased' | 'saw'\n",
        "\"\"\")\n",
        "\n",
        "# Создаём синтаксический парсер\n",
        "parser = ChartParser(grammar)\n",
        "\n",
        "# Пример предложения\n",
        "sentence = ['the', 'cat', 'chased', 'the', 'dog']\n",
        "\n",
        "# Анализируем предложение\n",
        "for tree in parser.parse(sentence):\n",
        "    print(tree)\n",
        "    tree.pretty_print()"
      ],
      "metadata": {
        "id": "vnOYN10IK_zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ecd99c-60bd-487c-a2d4-f4dc7f531872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP (Det the) (N cat)) (VP (V chased) (NP (Det the) (N dog))))\n",
            "              S               \n",
            "      ________|_____           \n",
            "     |              VP        \n",
            "     |         _____|___       \n",
            "     NP       |         NP    \n",
            "  ___|___     |      ___|___   \n",
            "Det      N    V    Det      N \n",
            " |       |    |     |       |  \n",
            "the     cat chased the     dog\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы определяем простую контекстно-свободную грамматику (CFG) и используем её для построения синтаксического дерева для предложения \"the cat chased the dog\". Это полезно для глубокого синтаксического анализа и обработки сложных предложений.\n",
        "\n",
        "### 6. **Извлечение именованных сущностей (NER)**\n",
        "\n",
        "Извлечение именованных сущностей (Named Entity Recognition, NER) — это задача, направленная на нахождение и классификацию сущностей, таких как имена людей, места, организации и т.д. NLTK предоставляет средства для выполнения этой задачи с помощью обученных моделей.\n",
        "\n",
        "#### Пример извлечения именованных сущностей"
      ],
      "metadata": {
        "id": "t3w0DW5vK_9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "sentence = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "words = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Извлечение именованных сущностей\n",
        "entities = ne_chunk(pos_tags)\n",
        "print(entities)"
      ],
      "metadata": {
        "id": "4_wzV6ngLAPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e63abc-7e55-4d17-b19b-c7492f47a8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Apple/NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы используем функцию `ne_chunk()`, чтобы извлечь именованные сущности из предложения. Этот метод полезен для задач анализа новостных статей, бизнес-аналитики и других текстов, где важно выделить ключевые объекты.\n",
        "\n",
        "### 7. **Тематическое моделирование (Topic Modeling)**\n",
        "\n",
        "Тематическое моделирование — это задача, направленная на выявление тем, к которым относится набор документов. NLTK можно использовать для таких задач в сочетании с библиотеками вроде `gensim`.\n",
        "\n",
        "#### Пример тематического моделирования с NLTK"
      ],
      "metadata": {
        "id": "g3Zf5Zo0LAXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "from gensim import corpora, models\n",
        "\n",
        "nltk.download('reuters')\n",
        "\n",
        "# Подготовка данных\n",
        "documents = [list(reuters.words(fileid)) for fileid in reuters.fileids()]\n",
        "\n",
        "# Создание словаря\n",
        "dictionary = corpora.Dictionary(documents)\n",
        "\n",
        "# Создание мешка слов (Bag of Words)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
        "\n",
        "# Обучение модели LDA (Latent Dirichlet Allocation)\n",
        "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
        "\n",
        "# Вывод тем\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "wWoXJ-8tLAmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8da5f0-2f21-4a01-a03f-588fe68a0279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.108*\",\" + 0.040*\".\" + 0.030*\"000\" + 0.027*\"tonnes\" + 0.021*\"of\"')\n",
            "(1, '0.098*\",\" + 0.070*\"vs\" + 0.055*\".\" + 0.042*\"mln\" + 0.037*\"000\"')\n",
            "(2, '0.194*\".\" + 0.054*\"mln\" + 0.036*\"billion\" + 0.032*\"1\" + 0.027*\"-\"')\n",
            "(3, '0.052*\"the\" + 0.044*\".\" + 0.029*\",\" + 0.025*\"to\" + 0.019*\"of\"')\n",
            "(4, '0.029*\"TO\" + 0.022*\".\" + 0.020*\"DLRS\" + 0.020*\"MLN\" + 0.020*\"SAYS\"')\n",
            "(5, '0.050*\".\" + 0.044*\"the\" + 0.041*\",\" + 0.039*\"in\" + 0.027*\"to\"')\n",
            "(6, '0.070*\".\" + 0.034*\"the\" + 0.033*\"pct\" + 0.029*\"in\" + 0.026*\",\"')\n",
            "(7, '0.056*\"cts\" + 0.033*\"April\" + 0.027*\">\" + 0.027*\";\" + 0.027*\"&\"')\n",
            "(8, '0.047*\".\" + 0.036*\"the\" + 0.035*\",\" + 0.033*\"of\" + 0.025*\"said\"')\n",
            "(9, '0.053*\"the\" + 0.049*\".\" + 0.031*\",\" + 0.031*\"to\" + 0.022*\"of\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:** Мы используем корпус Reuters для построения модели LDA, которая выделяет 10 тем в текстах. Тематическое моделирование полезно для анализа больших объемов данных, таких как новости, статьи или форумы.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "NLTK — это мощная и гибкая библиотека, которая поддерживает широкий спектр задач в области обработки естественного языка (NLP). Мы рассмотрели множество дополнительных возможностей NLTK, таких как работа с корпусами, анализ коллокаций, классификация текста, синтаксический анализ, извлечение именованных сущностей и тематическое моделирование."
      ],
      "metadata": {
        "id": "oaeVKNFtLAv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашка"
      ],
      "metadata": {
        "id": "H2M8F5dZLluo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Фильтрация стоп-слов: Создайте функцию, которая удаляет стоп-слова из заданного текста."
      ],
      "metadata": {
        "id": "WIsE1m6zLlke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Работа с корпусами: Загрузите текст из корпуса Gutenberg и выведите первые 500 символов."
      ],
      "metadata": {
        "id": "DYHJqTnxLviE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Сгенерировать биграмы для какого-нибудь татарского текста *"
      ],
      "metadata": {
        "id": "7nS11KuiQnOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вопросы для самопроверки\n",
        "\n",
        "1. Что такое NLTK и для чего она используется?\n",
        "\n",
        "2. Как установить библиотеку NLTK и какие ресурсы необходимо загрузить для начала работы?\n",
        "\n",
        "3. Что такое токенизация и какие функции NLTK используются для этого?\n",
        "\n",
        "4. Как можно фильтровать стоп-слова в тексте с помощью NLTK?\n",
        "\n",
        "5. В чем разница между стеммингом и лемматизацией?\n",
        "\n",
        "6. Какой стеммер используется в NLTK для русского языка?\n",
        "\n",
        "7. Как выполнить POS-тегирование в NLTK и что такое POS?\n",
        "\n",
        "8. Что такое корпус в NLTK и как его можно использовать?\n",
        "\n",
        "9. Как создать биграммы из текста с помощью NLTK?\n",
        "\n",
        "10. Что такое коллокации и как их можно найти с помощью NLTK?\n",
        "\n",
        "11. Как работает наивный байесовский классификатор в NLTK?\n",
        "\n",
        "12. Что такое синтаксический анализ и как его реализовать с помощью NLTK?\n",
        "\n",
        "13. Как извлечь именованные сущности из текста с использованием NLTK?\n",
        "\n",
        "14. Что такое тематическое моделирование и как его можно реализовать с помощью NLTK?\n",
        "\n",
        "15. Как можно визуализировать частоту слов с помощью Matplotlib?\n",
        "\n",
        "16. Какой метод можно использовать для обработки многозначных слов в NLTK?\n",
        "\n",
        "17. Как создать свою собственную функцию для извлечения признаков из текста?\n",
        "\n",
        "18. Что такое тематическое моделирование и как оно помогает в анализе текстов?\n",
        "\n",
        "19. Как можно использовать NLTK для создания чат-бота?\n",
        "\n",
        "20. Какие основные шаги нужно выполнить для анализа новостей с использованием NLTK?\n"
      ],
      "metadata": {
        "id": "TAU9NWG1dw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задачи дя самостоятеьной работы\n",
        "\n",
        "\n",
        "1. **Установка NLTK**: Установите библиотеку NLTK и загрузите необходимые ресурсы (punkt, wordnet, stopwords).\n",
        "\n",
        "2. **Токенизация**: Напишите программу, которая разбивает текст на предложения и слова на русском и английском языках.\n",
        "\n",
        "3. **Фильтрация стоп-слов**: Создайте функцию, которая удаляет стоп-слова из заданного текста.\n",
        "\n",
        "4. **Стемминг**: Реализуйте программу, которая стеммирует список слов на русском языке с использованием SnowballStemmer.\n",
        "\n",
        "5. **Лемматизация**: Напишите код для лемматизации списка слов на английском языке с использованием WordNetLemmatizer.\n",
        "\n",
        "6. **POS-тегирование**: Создайте программу для тегирования частей речи в предложении на русском языке.\n",
        "\n",
        "7. **Работа с корпусами**: Загрузите текст из корпуса Gutenberg и выведите первые 500 символов.\n",
        "\n",
        "8. **Создание N-грамм**: Реализуйте функцию, которая генерирует биграммы из заданного предложения.\n",
        "\n",
        "9. **Анализ коллокаций**: Напишите программу, которая находит наиболее частые биграммы в тексте.\n",
        "\n",
        "10. **Классификация текста**: Используйте наивный байесовский классификатор для классификации отзывов о фильмах.\n",
        "\n",
        "11. **Синтаксический анализ**: Создайте синтаксическое дерево для простого предложения с использованием контекстно-свободной грамматики.\n",
        "\n",
        "12. **Извлечение именованных сущностей**: Реализуйте программу, которая извлекает именованные сущности из текста.\n",
        "\n",
        "13. **Тематика текста**: Используйте тематическое моделирование для анализа набора документов с помощью LDA.\n",
        "\n",
        "14. **Частотный анализ**: Напишите код, который подсчитывает частоту слов в тексте и выводит 10 самых частых.\n",
        "\n",
        "15. **Сравнение стемминга и лемматизации**: Сравните результаты стемминга и лемматизации для одного и того же списка слов.\n",
        "\n",
        "16. **Токенизация с использованием регулярных выражений**: Реализуйте токенизацию текста с использованием регулярных выражений.\n",
        "\n",
        "17. **Создание собственного списка стоп-слов**: Создайте свой собственный список стоп-слов и используйте его для фильтрации текста.\n",
        "\n",
        "18. **Анализ тональности**: Реализуйте анализ тональности текста, используя классификатор, обученный на отзывах.\n",
        "\n",
        "19. **Построение графиков частоты слов**: Используйте Matplotlib для визуализации частоты слов в тексте.\n",
        "\n",
        "20. **Обработка больших текстов**: Напишите программу, которая будет обрабатывать большие тексты, используя итераторы.\n",
        "\n",
        "21. **Извлечение фраз**: Реализуйте функцию для извлечения фраз из текста на основе POS-тегирования.\n",
        "\n",
        "22. **Работа с многозначными словами**: Напишите код, который обрабатывает многозначные слова и выбирает правильное значение в контексте.\n",
        "\n",
        "23. **Создание собственных признаков для классификации**: Разработайте свою функцию для извлечения признаков из текста для классификации.\n",
        "\n",
        "24. **Интерактивный анализ текста**: Создайте интерактивную программу, которая позволяет пользователю вводить текст для анализа.\n",
        "\n",
        "25. **Сравнение различных стеммеров**: Сравните результаты стемминга с использованием разных стеммеров (Porter, Snowball).\n",
        "\n",
        "26. **Обработка диалогов**: Реализуйте анализ диалогов, используя NLTK для извлечения участников и их реплик.\n",
        "\n",
        "27. **Сложные синтаксические структуры**: Напишите код для анализа сложных синтаксических структур и их визуализации.\n",
        "\n",
        "28. **Создание текстового корпуса**: Соберите собственный текстовый корпус и выполните анализ частоты слов.\n",
        "\n",
        "29. **Разработка чат-бота**: Используйте NLTK для создания простого чат-бота, который понимает команды пользователя.\n",
        "\n",
        "30. **Обработка и анализ новостей**: Напишите программу, которая извлекает и анализирует новости из RSS-каналов.\n"
      ],
      "metadata": {
        "id": "V5sIFO9Sdw17"
      }
    }
  ]
}