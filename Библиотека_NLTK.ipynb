{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1HGmN+Vx9JjsdE5Ila+Pu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/A_PythonLibraries/blob/main/%D0%91%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Библиотека NLTK\n",
        "\n",
        "## Введение\n",
        "\n",
        "Библиотека NLTK (Natural Language Toolkit) — это мощный инструмент для работы с обработкой естественного языка (NLP) на языке Python. Она предоставляет возможность анализа текстов, создания и использования различных языковых ресурсов и инструментов, что делает её идеальным выбором для исследователей, разработчиков и студентов, работающих в области языковых технологий.\n",
        "\n",
        "В этой лекции мы рассмотрим:\n",
        "- Основные компоненты NLTK.\n",
        "- Как устанавливать NLTK и использовать его.\n",
        "- Различные функции и возможности библиотеки.\n",
        "- Примеры кода, демонстрирующие использование NLTK.\n",
        "\n",
        "## Установка NLTK\n",
        "\n",
        "Для начала работы с NLTK необходимо установить библиотеку. Это можно сделать с помощью менеджера пакетов pip. Откройте терминал и выполните следующую команду:\n",
        "\n",
        "```bash\n",
        "pip install nltk\n",
        "```\n",
        "\n",
        "После установки NLTK, для использования некоторых ресурсов и данных библиотеки, необходимо загрузить их. Для этого запустите следующий код в Python:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download()\n",
        "```\n",
        "\n",
        "Эта команда откроет графический интерфейс, из которого вы сможете выбрать необходимые ресурсы для загрузки.\n",
        "\n",
        "## Основные компоненты NLTK\n",
        "\n",
        "NLTK состоит из нескольких важных компонентов, среди которых:\n",
        "- **Корпуса** — коллекции текстов на разных языках.\n",
        "- **Токенизация** — процесс разбивки текста на отдельные элементы (слова, предложения и т.д.).\n",
        "- **Частеречная разметка** — определение частей речи для каждого слова в тексте.\n",
        "- **Стемминг и лемматизация** — преобразование слов к их базовым формам.\n",
        "- **Синтаксический и семантический анализ** — анализ структуры и смысла предложений.\n",
        "- **Модели для машинного обучения** — инструменты для построения и использования моделей NLP.\n",
        "\n",
        "## 1. Токенизация\n",
        "\n",
        "### Что такое токенизация?\n",
        "\n",
        "Токенизация — это процесс разбивки текста на отдельные элементы, называемые токенами. Токены могут быть словами, предложениями или даже отдельными символами. Токенизация является одним из первых шагов в обработке естественного языка.\n",
        "\n",
        "### Пример токенизации предложений и слов\n",
        "\n",
        "В следующем примере мы покажем, как использовать NLTK для токенизации текста на предложения и слова.\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Пример текста\n",
        "text = \"Привет! Как дела? Это пример текста для токенизации.\"\n",
        "\n",
        "# Токенизация предложений\n",
        "sentences = sent_tokenize(text, language='russian')\n",
        "print(\"Предложения:\", sentences)\n",
        "\n",
        "# Токенизация слов\n",
        "words = word_tokenize(text, language='russian')\n",
        "print(\"Слова:\", words)\n",
        "```\n",
        "\n",
        "### Объяснение кода\n",
        "\n",
        "1. Импортируем необходимые модули из NLTK.\n",
        "2. Определяем текст, который мы хотим токенизировать.\n",
        "3. Используем `sent_tokenize` для разбиения текста на предложения.\n",
        "4. Используем `word_tokenize` для разбиения текста на слова.\n",
        "5. Выводим результаты на экран.\n",
        "\n",
        "## 2. Частеречная разметка\n",
        "\n",
        "### Что такое частеречная разметка?\n",
        "\n",
        "Частеречная разметка — это процесс определения частей речи (существительное, глагол, прилагательное и т.д.) для каждого слова в тексте. Это помогает понять грамматическую структуру текста и значение слов.\n",
        "\n",
        "### Пример частеречной разметки\n",
        "\n",
        "В следующем примере мы покажем, как использовать NLTK для частеречной разметки.\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Пример текста\n",
        "text = \"Собака бегает по парку.\"\n",
        "\n",
        "# Токенизация слов\n",
        "words = word_tokenize(text, language='russian')\n",
        "\n",
        "# Частеречная разметка\n",
        "tagged_words = pos_tag(words, lang='rus')\n",
        "print(\"Частеречная разметка:\", tagged_words)\n",
        "```\n",
        "\n",
        "### Объяснение кода\n",
        "\n",
        "1. Импортируем необходимые модули.\n",
        "2. Определяем текст, который мы хотим разобрать.\n",
        "3. Используем `word_tokenize` для токенизации текста на слова.\n",
        "4. Применяем `pos_tag` для разметки слов по частям речи.\n",
        "5. Выводим разметку на экран.\n",
        "\n",
        "## 3. Стемминг и лемматизация\n",
        "\n",
        "### Что такое стемминг и лемматизация?\n",
        "\n",
        "- **Стемминг** — это процесс сокращения слова до его корня (стема). Например, слова \"бегать\", \"бегу\", \"бежал\" могут быть приведены к стему \"бег\".\n",
        "- **Лемматизация** — это более сложный процесс, который включает в себя преобразование слова в его базовую форму (лемму), принимая во внимание его значение и грамматическую форму. Например, слово \"бегал\" преобразуется в \"бегать\".\n",
        "\n",
        "### Пример стемминга и лемматизации\n",
        "\n",
        "В NLTK есть встроенные инструменты для стемминга и лемматизации, но лемматизация требует дополнительных ресурсов.\n",
        "\n",
        "```python\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Создание объектов стеммера и лемматизатора\n",
        "stemmer = SnowballStemmer(language='russian')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Примеры слов\n",
        "words = [\"бегаю\", \"бегал\", \"бегу\"]\n",
        "\n",
        "# Стемминг\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Стемминг:\", stemmed_words)\n",
        "\n",
        "# Лемматизация (для демонстрации)\n",
        "# В NLTK нет русской лемматизации, поэтому здесь используется английский пример\n",
        "english_words = [\"running\", \"ran\", \"runs\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in english_words]\n",
        "print(\"Лемматизация:\", lemmatized_words)\n",
        "```\n",
        "\n",
        "### Объяснение кода\n",
        "\n",
        "1. Импортируем стеммер и лемматизатор.\n",
        "2. Создаём объекты для стемминга и лемматизации.\n",
        "3. Определяем список слов для стемминга.\n",
        "4. Применяем стемминг к каждому слову с помощью спискового включения.\n",
        "5. Для демонстрации лемматизации используем английские слова.\n",
        "6. Выводим результаты на экран.\n",
        "\n",
        "## 4. Синтаксический анализ\n",
        "\n",
        "### Что такое синтаксический анализ?\n",
        "\n",
        "Синтаксический анализ включает в себя разбор предложений на составные части и выявление их грамматической структуры. Это позволяет понять, как слова в предложении связаны между собой.\n",
        "\n",
        "### Пример синтаксического анализа\n",
        "\n",
        "В NLTK можно использовать парсеры для синтаксического анализа. Вот пример простого синтаксического анализа:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk import CFG\n",
        "\n",
        "# Определение контекстно-свободной грамматики\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det N\n",
        "    VP -> V NP\n",
        "    Det -> 'в' | 'один'\n",
        "    N -> 'парк' | 'собака'\n",
        "    V -> 'бегает'\n",
        "\"\"\")\n",
        "\n",
        "# Пример предложения\n",
        "sentence = 'один собака бегает в парк'.split()\n",
        "\n",
        "# Синтаксический анализ\n",
        "parser = nltk.ChartParser(grammar)\n",
        "for tree in parser.parse(sentence):\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n",
        "```\n",
        "\n",
        "### Объяснение кода\n",
        "\n",
        "1. Импортируем необходимые модули из NLTK.\n",
        "2. Определяем контекстно-свободную грамматику (CFG) для нашего примера.\n",
        "3. Задаём предложение, которое хотим разобрать.\n",
        "4. Создаём парсер и применяем его к предложению.\n",
        "5. Выводим синтаксическое дерево и его визуализацию.\n",
        "\n",
        "## 5. Модели для машинного обучения\n",
        "\n",
        "### Возможности машинного обучения в NLTK\n",
        "\n",
        "NLTK предоставляет возможность использовать различные алгоритмы машинного обучения для задач обработки естественного языка, включая классификацию текстов, анализ настроений и т.д.\n",
        "\n",
        "### Пример классификации текста\n",
        "\n",
        "В этом примере мы используем NLTK для классификации текста на основе простого набора данных.\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "# Загружаем отзывы о фильмах\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Перемешиваем документы\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Извлекаем слова\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]\n",
        "\n",
        "# Определяем функцию для извлечения признаков\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[f'contains({word})'] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "# Подготавливаем обучающие и тестовые наборы\n",
        "featuresets = [(document_features(doc), category) for (doc, category) in documents]\n",
        "train_set, test_set = featuresets[100:],\n",
        "\n",
        " featuresets[:100]\n",
        "\n",
        "# Обучаем классификатор\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Оцениваем точность классификатора\n",
        "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
        "\n",
        "# Печатаем наиболее информативные признаки\n",
        "classifier.show_most_informative_features(5)\n",
        "```\n",
        "\n",
        "### Объяснение кода\n",
        "\n",
        "1. Импортируем необходимые модули.\n",
        "2. Загружаем отзывы о фильмах из корпуса `movie_reviews`.\n",
        "3. Перемешиваем документы для создания случайного набора данных.\n",
        "4. Извлекаем 2000 наиболее частых слов в отзывах.\n",
        "5. Определяем функцию `document_features`, которая извлекает признаки из документа.\n",
        "6. Готовим обучающие и тестовые наборы.\n",
        "7. Обучаем классификатор Наивного Байеса на обучающем наборе.\n",
        "8. Оцениваем точность классификатора на тестовом наборе и выводим наиболее информативные признаки.\n",
        "\n",
        "\n",
        "#Дополнительные возможности NLTK\n",
        "\n",
        "\n",
        "\n",
        "## 1. Работа с корпусами текстов на русском языке\n",
        "\n",
        "### Использование русских текстов и корпусов\n",
        "\n",
        "Корпуса текстов — это большие коллекции текстов, которые могут использоваться для анализа, исследования и обучения. В NLTK можно создать собственный корпус или использовать уже доступные. К сожалению, стандартные корпуса в NLTK в основном ориентированы на английский язык, поэтому мы можем использовать внешние источники или создавать собственные.\n",
        "\n",
        "### Пример создания корпуса\n",
        "\n",
        "Для создания собственного корпуса на русском языке вы можете использовать `PlaintextCorpusReader`. Предположим, у вас есть папка с текстовыми файлами на русском.\n",
        "\n",
        "```python\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "# Укажите путь к папке с текстами\n",
        "corpus_root = 'путь/к/вашей/папке/с/текстами'\n",
        "corpus = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
        "\n",
        "# Просмотр файлов в корпусе\n",
        "print(corpus.fileids())\n",
        "\n",
        "# Чтение текста первого файла\n",
        "print(corpus.raw(corpus.fileids()[0]))\n",
        "```\n",
        "\n",
        "### Примеры доступных ресурсов\n",
        "\n",
        "Существуют открытые ресурсы, такие как [Национальный корпус русского языка](http://www.ruscorpora.ru) или [Тексты на русском языке на Kaggle](https://www.kaggle.com/datasets), которые можно использовать для создания корпусов.\n",
        "\n",
        "## 2. Расширенные методы токенизации\n",
        "\n",
        "### Настройка токенизаторов для русского языка\n",
        "\n",
        "Токенизация — это один из первых шагов в анализе текста. Для русского языка токенизация может быть немного сложнее, особенно если текст содержит аббревиатуры, числовые значения и специальные символы.\n",
        "\n",
        "### Пример токенизации с учетом специфики\n",
        "\n",
        "NLTK предоставляет стандартные методы токенизации, но вы можете создать свои токенизаторы для более точного анализа:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Создаем токенизатор, который разделяет текст по пробелам и учитывает только буквы и цифры\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "text = \"Привет! Сегодня 26 сентября 2024 года. Время: 14:30.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Токены:\", tokens)\n",
        "```\n",
        "\n",
        "### Обработка сложных случаев\n",
        "\n",
        "Для более сложных случаев можно использовать регулярные выражения, чтобы учитывать различные форматы, такие как аббревиатуры и числовые данные.\n",
        "\n",
        "```python\n",
        "import re\n",
        "\n",
        "# Пример текста с аббревиатурами\n",
        "text = \"Система АБС работает 24/7. Скорость: 100 км/ч.\"\n",
        "\n",
        "# Регулярное выражение для токенизации\n",
        "tokens = re.findall(r'\\w+', text)\n",
        "print(\"Токены с учетом аббревиатур:\", tokens)\n",
        "```\n",
        "\n",
        "## 3. Стемминг и лемматизация на русском языке\n",
        "\n",
        "### Специальные алгоритмы для русскоязычных текстов\n",
        "\n",
        "Для работы с русским языком часто используются алгоритмы стемминга и лемматизации. Как уже упоминалось, для стемминга можно использовать Snowball Stemmer, а для лемматизации — Pymorphy2.\n",
        "\n",
        "### Пример стемминга\n",
        "\n",
        "```python\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Создаем объект стеммера\n",
        "stemmer = SnowballStemmer(language='russian')\n",
        "\n",
        "# Примеры слов\n",
        "words = [\"бегаю\", \"бегал\", \"бег\"]\n",
        "\n",
        "# Стемминг слов\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Стемминг:\", stemmed_words)\n",
        "```\n",
        "\n",
        "### Пример лемматизации с использованием Pymorphy2\n",
        "\n",
        "```python\n",
        "import pymorphy2\n",
        "\n",
        "# Создаем объект морфологического анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Примеры слов для лемматизации\n",
        "words = [\"бегаю\", \"бегал\", \"бег\", \"бегут\"]\n",
        "\n",
        "# Лемматизация слов\n",
        "lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
        "print(\"Леммы:\", lemmas)\n",
        "```\n",
        "\n",
        "### Сравнение стемминга и лемматизации\n",
        "\n",
        "- **Стемминг**: более грубый подход, просто обрезает окончания.\n",
        "- **Лемматизация**: более точный метод, учитывающий значение слова и его грамматические формы.\n",
        "\n",
        "## 4. Обработка текстов с использованием регулярных выражений\n",
        "\n",
        "Регулярные выражения — мощный инструмент для предварительной обработки текстов, позволяющий находить и заменять определенные шаблоны.\n",
        "\n",
        "### Пример использования регулярных выражений\n",
        "\n",
        "```python\n",
        "import re\n",
        "\n",
        "# Пример текста\n",
        "text = \"Это пример текста с лишними символами!!! @#$%^&*()\"\n",
        "\n",
        "# Удаление специальных символов\n",
        "cleaned_text = re.sub(r'[^а-яА-Я0-9\\s]', '', text)\n",
        "print(\"Очистка текста:\", cleaned_text)\n",
        "\n",
        "# Удаление лишних пробелов\n",
        "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(\"Очистка текста от лишних пробелов:\", cleaned_text)\n",
        "```\n",
        "\n",
        "### Очистка текста от HTML-тегов\n",
        "\n",
        "Если у вас есть текст с HTML-разметкой, можно использовать регулярные выражения для удаления тегов:\n",
        "\n",
        "```python\n",
        "html_text = \"<p>Это пример текста <b>с жирным текстом</b>.</p>\"\n",
        "cleaned_text = re.sub(r'<.*?>', '', html_text)\n",
        "print(\"Текст без HTML:\", cleaned_text)\n",
        "```\n",
        "\n",
        "## 5. Классификация текстов и анализ настроений\n",
        "\n",
        "### Более сложные примеры машинного обучения\n",
        "\n",
        "Классификация текстов позволяет автоматически определять категорию текста. В NLTK можно обучать классификаторы с использованием различных алгоритмов, таких как Наивный Байес, SVM и т.д.\n",
        "\n",
        "### Пример классификации\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "# Загружаем отзывы о фильмах\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Перемешиваем документы\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Извлекаем слова\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]\n",
        "\n",
        "# Определяем функцию для извлечения признаков\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[f'contains({word})'] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "# Подготавливаем обучающие и тестовые наборы\n",
        "featuresets = [(document_features(doc), category) for (doc, category) in documents]\n",
        "train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "\n",
        "# Обучаем классификатор\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Оцениваем точность классификатора\n",
        "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
        "```\n",
        "\n",
        "### Применение моделей на основе глубокого обучения\n",
        "\n",
        "Для более сложных задач классификации, таких как анализ настроений, можно использовать библиотеки глубокого обучения, такие как TensorFlow или PyTorch, совместно с NLTK для предварительной обработки текстов.\n",
        "\n",
        "\n",
        "## 6. Синтаксический анализ и разбор предложений на русском языке\n",
        "\n",
        "Для синтаксического анализа текста на русском языке с использованием библиотеки `spaCy`, необходимо загрузить и установить русскоязычную модель. Далее модель применяется для разметки текста, после чего можно извлекать деревья зависимостей и другую полезную информацию о структуре предложений.\n",
        "\n",
        "### Продолжение примера синтаксического анализа с использованием `spaCy`\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Загружаем русскую модель\n",
        "nlp = spacy.load('ru_core_news_sm')\n",
        "\n",
        "# Пример предложения на русском\n",
        "text = \"Кошка сидит на подоконнике и смотрит в окно.\"\n",
        "\n",
        "# Обрабатываем текст с помощью spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Выводим морфологический анализ каждого токена\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_}, {token.dep_}, {token.head.text}\")\n",
        "```\n",
        "\n",
        "### Объяснение вывода\n",
        "\n",
        "- **token.text**: оригинальный текст токена.\n",
        "- **token.pos_**: часть речи токена (например, существительное, глагол и т.д.).\n",
        "- **token.dep_**: синтаксическая зависимость токена (например, подлежащее, дополнение).\n",
        "- **token.head**: головное слово, с которым этот токен связан (например, для дополнений — это глагол, к которому они относятся).\n",
        "\n",
        "### Пример вывода\n",
        "\n",
        "```\n",
        "Кошка: NOUN, nsubj, сидит\n",
        "сидит: VERB, ROOT, сидит\n",
        "на: ADP, case, подоконнике\n",
        "подоконнике: NOUN, obl, сидит\n",
        "и: CCONJ, cc, смотрит\n",
        "смотрит: VERB, conj, сидит\n",
        "в: ADP, case, окно\n",
        "окно: NOUN, obl, смотрит\n",
        ".: PUNCT, punct, сидит\n",
        "```\n",
        "\n",
        "В этом примере мы видим, что `Кошка` — это подлежащее (nsubj) для глагола `сидит` (главного глагола предложения), а `подоконнике` — это обстоятельство (obl) того же глагола.\n",
        "\n",
        "\n",
        "## 7. Работа с семантическим анализом\n",
        "\n",
        "Семантический анализ — это процесс извлечения смысла из текста. В этой области можно выделить два популярных подхода:\n",
        "\n",
        "- **Использование векторных представлений слов (Word Embeddings)**, таких как Word2Vec или FastText, которые помогают моделировать семантические связи между словами.\n",
        "- **Latent Semantic Analysis (LSA)** — метод для уменьшения размерности данных и выявления латентных (скрытых) семантических тем в корпусе текстов.\n",
        "\n",
        "### Примеры использования векторных моделей слов (Word2Vec, FastText)\n",
        "\n",
        "**Word2Vec** и **FastText** позволяют представлять слова в виде векторов в многомерном пространстве, где семантически близкие слова располагаются рядом. Эти методы хорошо работают для обработки больших объемов текста и полезны для таких задач, как кластеризация или определение семантической близости.\n",
        "\n",
        "#### Пример использования Word2Vec с Gensim\n",
        "\n",
        "Для работы с Word2Vec в Python часто используется библиотека `Gensim`, которая позволяет обучать модель на текстах.\n",
        "\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "# Пример текста для обучения\n",
        "sentences = [\n",
        "    \"собака лает\",\n",
        "    \"кошка мяукает\",\n",
        "    \"птица поет\",\n",
        "    \"машина едет\",\n",
        "    \"человек говорит\",\n",
        "]\n",
        "\n",
        "# Токенизация предложений\n",
        "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# Обучение модели Word2Vec\n",
        "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Поиск наиболее похожих слов\n",
        "similar_words = model.wv.most_similar(\"собака\")\n",
        "print(\"Семантически близкие слова к 'собака':\", similar_words)\n",
        "```\n",
        "\n",
        "**FastText** — это расширение Word2Vec, которое использует информацию о символах и может обрабатывать редкие слова или незнакомые формы. Он особенно полезен для работы с русским языком.\n",
        "\n",
        "#### Пример использования FastText\n",
        "\n",
        "```python\n",
        "from gensim.models import FastText\n",
        "\n",
        "# Обучение модели FastText\n",
        "fasttext_model = FastText(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Поиск похожих слов\n",
        "similar_words_ft = fasttext_model.wv.most_similar(\"кошка\")\n",
        "print(\"Семантически близкие слова к 'кошка':\", similar_words_ft)\n",
        "```\n",
        "\n",
        "### LSA (Latent Semantic Analysis)\n",
        "\n",
        "LSA используется для анализа больших текстовых коллекций и выявления скрытых тем. Этот метод полезен для задачи тематического моделирования.\n",
        "\n",
        "#### Пример тематического моделирования с использованием LSA\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Пример текстов\n",
        "documents = [\n",
        "    \"собака лает на кошку\",\n",
        "    \"кошка мяукает и убегает\",\n",
        "    \"собака гонится за кошкой\",\n",
        "    \"кошка прячется от собаки\"\n",
        "]\n",
        "\n",
        "# Преобразование текстов в мешок слов\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Применение LSA для извлечения двух латентных тем\n",
        "lsa = TruncatedSVD(n_components=2)\n",
        "lsa_result = lsa.fit_transform(X)\n",
        "\n",
        "print(\"Темы в текстах:\")\n",
        "print(lsa_result)\n",
        "```\n",
        "\n",
        "## 8. Примеры реальных приложений NLTK\n",
        "\n",
        "Теперь рассмотрим, как NLTK может применяться в реальных задачах, таких как создание чат-ботов, системы рекомендаций и анализ отзывов.\n",
        "\n",
        "### 1. Создание простого чат-бота\n",
        "\n",
        "NLTK можно использовать для создания чат-ботов с простыми правилами и обработкой естественного языка. Вот пример на основе ключевых слов и шаблонов.\n",
        "\n",
        "```python\n",
        "import random\n",
        "import nltk\n",
        "\n",
        "# Набор примитивных шаблонов для общения\n",
        "responses = {\n",
        "    'привет': ['Привет!', 'Здравствуйте!', 'Добрый день!'],\n",
        "    'как дела': ['Хорошо, спасибо. А у вас?', 'Все отлично!', 'Дела идут!'],\n",
        "    'пока': ['До свидания!', 'Пока!', 'Всего хорошего!'],\n",
        "}\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    # Токенизация пользовательского ввода\n",
        "    tokens = nltk.word_tokenize(user_input.lower())\n",
        "    \n",
        "    # Поиск подходящего ответа по ключевым словам\n",
        "    for token in tokens:\n",
        "        if token in responses:\n",
        "            return random.choice(responses[token])\n",
        "    \n",
        "    return \"Извините, я вас не понял.\"\n",
        "\n",
        "# Пример использования бота\n",
        "user_input = \"Привет\"\n",
        "print(\"Чат-бот:\", chatbot_response(user_input))\n",
        "```\n",
        "\n",
        "### 2. Реализация системы рекомендаций на основе анализа текстов\n",
        "\n",
        "Системы рекомендаций могут использовать текстовую информацию для создания персонализированных рекомендаций. Например, на основе анализа предпочтений пользователя и текстов товаров (или фильмов) можно рекомендовать похожие объекты.\n",
        "\n",
        "Примером может быть использование анализа настроений для фильтрации отзывов или обучение модели на отзывах о фильмах:\n",
        "\n",
        "```python\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.classify import accuracy\n",
        "\n",
        "# Загружаем отзывы о фильмах\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Перемешиваем и разбиваем данные на обучение и тестирование\n",
        "random.shuffle(documents)\n",
        "train_set = documents[:1600]\n",
        "test_set = documents[1600:]\n",
        "\n",
        "# Обучаем классификатор\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Оцениваем точность модели\n",
        "print(\"Точность:\", accuracy(classifier, test_set))\n",
        "\n",
        "# Печатаем наиболее информативные признаки\n",
        "classifier.show_most_informative_features(5)\n",
        "```\n",
        "\n",
        "## Заключение\n",
        "\n",
        "NLTK — это мощный инструмент для обработки естественного языка, предлагающий множество возможностей, начиная от токенизации и частеречной разметки до синтаксического анализа и классификации текстов. В этой лекции мы рассмотрели основные компоненты и примеры использования NLTK, что поможет вам начать работать с этой библиотекой.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ttYyB4_vHF4V"
      }
    }
  ]
}