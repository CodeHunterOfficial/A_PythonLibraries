{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF6dP51kl8fMya0xtOaIfv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/A_PythonLibraries/blob/main/%D0%91%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Библиотека NLTK\n",
        "\n",
        "## Введение в NLTK\n",
        "\n",
        "**NLTK (Natural Language Toolkit)** — это мощная библиотека Python для обработки естественного языка (NLP). Она предоставляет множество инструментов и ресурсов для работы с текстом, включая токенизацию, стемминг, лемматизацию, разбор синтаксиса и многие другие задачи. Библиотека особенно полезна для студентов, исследователей и разработчиков, работающих в области обработки текста и создания языковых моделей.\n",
        "\n",
        "### Установка NLTK\n",
        "\n",
        "Перед тем как приступить к практическим примерам, необходимо установить библиотеку NLTK. Для этого выполните следующую команду в терминале или командной строке:\n",
        "\n",
        "```bash\n",
        "pip install nltk\n",
        "```\n",
        "\n",
        "После установки мы можем импортировать библиотеку и загрузить необходимые ресурсы.\n",
        "\n",
        "### Импортирование NLTK и загрузка ресурсов\n",
        "\n",
        "NLTK предоставляет множество корпусов и ресурсов, которые можно загрузить. Для начала мы импортируем библиотеку и загрузим набор данных.\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')  # Для токенизации\n",
        "nltk.download('wordnet')  # Для лемматизации\n",
        "nltk.download('stopwords')  # Для работы со стоп-словами\n",
        "```\n",
        "\n",
        "## Токенизация\n",
        "\n",
        "Токенизация — это процесс разделения текста на составные части, такие как слова или предложения. В NLTK это делается с помощью функции `nltk.word_tokenize()` для слов и `nltk.sent_tokenize()` для предложений.\n",
        "\n",
        "### Пример токенизации\n",
        "\n",
        "#### На русском\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Привет, как дела? Сегодня прекрасный день!\"\n",
        "sentences = sent_tokenize(text, language='russian')\n",
        "words = word_tokenize(text, language='russian')\n",
        "\n",
        "print(\"Предложения:\", sentences)\n",
        "print(\"Слова:\", words)\n",
        "```\n",
        "\n",
        "**Объяснение:** В данном примере мы разделяем текст на предложения и слова. Функция `sent_tokenize()` разбивает текст на предложения, а `word_tokenize()` — на отдельные слова.\n",
        "\n",
        "#### In English\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello, how are you? It's a beautiful day today!\"\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n",
        "```\n",
        "\n",
        "**Explanation:** Here, we split the text into sentences and words using the same two functions. The output will show how the text is broken down.\n",
        "\n",
        "## Стоп-слова\n",
        "\n",
        "Стоп-слова — это слова, которые часто встречаются в тексте, но не несут значительной смысловой нагрузки (например, \"и\", \"в\", \"на\"). В NLTK можно получить список стоп-слов для различных языков.\n",
        "\n",
        "### Пример работы со стоп-словами\n",
        "\n",
        "#### На русском\n",
        "\n",
        "```python\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "text = \"Это простой пример текста с некоторыми стоп-словами.\"\n",
        "words = word_tokenize(text, language='russian')\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Исходные слова:\", words)\n",
        "print(\"Фильтрованные слова:\", filtered_words)\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы загружаем список стоп-слов на русском языке и используем его для фильтрации слов из нашего текста, оставляя только те, которые имеют смысл.\n",
        "\n",
        "#### In English\n",
        "\n",
        "```python\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "text = \"This is a simple example of a text with some stop words.\"\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "```\n",
        "\n",
        "**Explanation:** In this example, we load the list of stop words in English and filter them out from the text, keeping only meaningful words.\n",
        "\n",
        "## Стемминг и Лемматизация\n",
        "\n",
        "### Стемминг\n",
        "\n",
        "Стемминг — это процесс уменьшения слов до их корня, что может быть полезно для анализа текстов. В NLTK есть несколько стеммеров, таких как `PorterStemmer`.\n",
        "\n",
        "#### Пример стемминга\n",
        "\n",
        "#### На русском\n",
        "\n",
        "```python\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "words = [\"бегать\", \"бегал\", \"бегущий\", \"бегу\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Стеммированные слова:\", stemmed_words)\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы используем стеммер для русского языка, чтобы получить корни различных форм слова \"бегать\".\n",
        "\n",
        "#### In English\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"ran\", \"runner\", \"runs\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "```\n",
        "\n",
        "**Explanation:** Here, we use the Porter stemmer to reduce different forms of the word \"run\" to their root.\n",
        "\n",
        "### Лемматизация\n",
        "\n",
        "Лемматизация — более сложный процесс, который приводит слово к его начальной форме (лемме). В NLTK это можно сделать с помощью `WordNetLemmatizer`.\n",
        "\n",
        "#### Пример лемматизации\n",
        "\n",
        "#### На русском\n",
        "\n",
        "```python\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"бегал\", \"бегущий\", \"бегу\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(\"Лемматизированные слова:\", lemmatized_words)\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы используем лемматизатор для получения начальной формы слова. Здесь мы указываем, что слова — это глаголы.\n",
        "\n",
        "#### In English\n",
        "\n",
        "```python\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"ran\", \"runner\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n",
        "```\n",
        "\n",
        "**Explanation:** In this example, we use the lemmatizer to convert different forms of the verb \"run\" into its base form.\n",
        "\n",
        "## Часть речи (POS) Тегирование\n",
        "\n",
        "Тегирование частей речи — это процесс обозначения слов в тексте их соответствующими частями речи (существительное, глагол и т. д.). NLTK предоставляет инструменты для этого.\n",
        "\n",
        "### Пример POS-тегирования\n",
        "\n",
        "#### На русском\n",
        "\n",
        "```python\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('cess_esp')\n",
        "\n",
        "text = \"Сегодня солнечный день.\"\n",
        "words = word_tokenize(text, language='russian')\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "print(\"Тегированные слова:\", tagged_words)\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы используем токенизацию для получения списка слов, а затем применяем тегирование частей речи для их обозначения.\n",
        "\n",
        "#### In English\n",
        "\n",
        "```python\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"Today is a sunny day.\"\n",
        "words = word_tokenize(text)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "print(\"Tagged words:\", tagged_words)\n",
        "```\n",
        "\n",
        "**Explanation:** We tokenize the text and then tag the words with their corresponding parts of speech.\n",
        "\n",
        "Библиотека NLTK (Natural Language Toolkit) обладает множеством функций, выходящих далеко за рамки базовой обработки текста, такой как токенизация, стемминг или работа со стоп-словами. Рассмотрим дополнительные возможности NLTK, которые также могут быть полезны при решении задач обработки естественного языка.\n",
        "\n",
        "### 1. **Работа с корпусами и текстовыми коллекциями**\n",
        "\n",
        "NLTK содержит большой набор **корпусов** (коллекций текстов), которые можно использовать для обучения моделей, тестирования алгоритмов и экспериментов с обработкой текстов. Корпуса NLTK включают в себя книги, статьи, аннотации и даже примеры диалогов.\n",
        "\n",
        "#### Пример работы с корпусом\n",
        "\n",
        "```python\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Загрузка текста \"Гамлет\" из корпуса\n",
        "hamlet_text = gutenberg.raw('shakespeare-hamlet.txt')\n",
        "\n",
        "# Вывод первых 500 символов\n",
        "print(hamlet_text[:500])\n",
        "```\n",
        "\n",
        "**Объяснение:** В этом примере мы используем корпус **Gutenberg**, чтобы получить текст \"Гамлет\" Уильяма Шекспира. Корпус NLTK содержит много классических текстов и полезен для анализа литературы или других текстов.\n",
        "\n",
        "### 2. **N-граммы и моделирование текста**\n",
        "\n",
        "**N-граммы** — это последовательности из N слов, которые используются для моделирования языка, анализа текста и построения языковых моделей. NLTK позволяет легко генерировать и анализировать такие последовательности.\n",
        "\n",
        "#### Пример создания N-грамм\n",
        "\n",
        "```python\n",
        "from nltk import ngrams\n",
        "\n",
        "sentence = \"Программирование на Python с использованием NLTK\"\n",
        "words = word_tokenize(sentence, language='russian')\n",
        "\n",
        "# Генерация биграмм (2-грамм)\n",
        "bigrams = list(ngrams(words, 2))\n",
        "\n",
        "print(\"Биграммы:\", bigrams)\n",
        "```\n",
        "\n",
        "**Объяснение:** В данном примере мы создаём биграммы — последовательности из двух слов. N-граммы полезны для задач предсказания текста, анализа коллокаций (часто встречающихся пар слов) и машинного перевода.\n",
        "\n",
        "### 3. **Анализ коллокаций**\n",
        "\n",
        "Коллокации — это часто встречающиеся сочетания слов, такие как \"сильный дождь\" или \"зеленый свет\". NLTK предоставляет механизмы для поиска таких комбинаций в тексте.\n",
        "\n",
        "#### Пример анализа коллокаций\n",
        "\n",
        "```python\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "\n",
        "text = word_tokenize(\"NLTK is a leading platform for building Python programs to work with human language data.\", language='english')\n",
        "\n",
        "# Найдём часто встречающиеся биграммы\n",
        "finder = BigramCollocationFinder.from_words(text)\n",
        "collocations = finder.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n",
        "\n",
        "print(\"Коллокации:\", collocations)\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы используем `BigramCollocationFinder`, чтобы найти наиболее частотные биграммы в тексте. Анализ коллокаций полезен для улучшения качества текста и работы с лексическими моделями.\n",
        "\n",
        "### 4. **Классификация текста**\n",
        "\n",
        "NLTK содержит инструменты для классификации текста на основе машинного обучения. Это полезно для задач, таких как классификация тональности (например, положительный или отрицательный отзыв), классификация новостей и т.д. NLTK предоставляет несколько классификаторов, включая наивный байесовский классификатор.\n",
        "\n",
        "#### Пример классификации текста\n",
        "\n",
        "```python\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "# Подготовка данных\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Извлечение признаков\n",
        "def document_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for word in movie_reviews.words():\n",
        "        features[word] = (word in words)\n",
        "    return features\n",
        "\n",
        "# Подготовка тренировочных данных\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "\n",
        "# Обучение наивного байесовского классификатора\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Тестирование классификатора\n",
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print(f\"Точность классификации: {accuracy * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы используем **наивный байесовский классификатор**, чтобы классифицировать отзывы о фильмах как положительные или отрицательные. NLTK позволяет легко строить модели классификации для различных задач анализа текста.\n",
        "\n",
        "### 5. **Синтаксический анализ (Parsing)**\n",
        "\n",
        "Синтаксический анализ (или разбор) используется для построения синтаксических деревьев, которые показывают структуру предложения. Это важно для задач глубинного анализа текста, таких как извлечение сущностей и отношение между частями предложения.\n",
        "\n",
        "#### Пример синтаксического анализа\n",
        "\n",
        "```python\n",
        "from nltk import CFG\n",
        "from nltk import ChartParser\n",
        "\n",
        "# Определяем контекстно-свободную грамматику\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  NP -> Det N\n",
        "  VP -> V NP\n",
        "  Det -> 'the'\n",
        "  N -> 'cat' | 'dog'\n",
        "  V -> 'chased' | 'saw'\n",
        "\"\"\")\n",
        "\n",
        "# Создаём синтаксический парсер\n",
        "parser = ChartParser(grammar)\n",
        "\n",
        "# Пример предложения\n",
        "sentence = ['the', 'cat', 'chased', 'the', 'dog']\n",
        "\n",
        "# Анализируем предложение\n",
        "for tree in parser.parse(sentence):\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы определяем простую контекстно-свободную грамматику (CFG) и используем её для построения синтаксического дерева для предложения \"the cat chased the dog\". Это полезно для глубокого синтаксического анализа и обработки сложных предложений.\n",
        "\n",
        "### 6. **Извлечение именованных сущностей (NER)**\n",
        "\n",
        "Извлечение именованных сущностей (Named Entity Recognition, NER) — это задача, направленная на нахождение и классификацию сущностей, таких как имена людей, места, организации и т.д. NLTK предоставляет средства для выполнения этой задачи с помощью обученных моделей.\n",
        "\n",
        "#### Пример извлечения именованных сущностей\n",
        "\n",
        "```python\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "\n",
        "sentence = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "words = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Извлечение именованных сущностей\n",
        "entities = ne_chunk(pos_tags)\n",
        "print(entities)\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы используем функцию `ne_chunk()`, чтобы извлечь именованные сущности из предложения. Этот метод полезен для задач анализа новостных статей, бизнес-аналитики и других текстов, где важно выделить ключевые объекты.\n",
        "\n",
        "### 7. **Тематическое моделирование (Topic Modeling)**\n",
        "\n",
        "Тематическое моделирование — это задача, направленная на выявление тем, к которым относится набор документов. NLTK можно использовать для таких задач в сочетании с библиотеками вроде `gensim`.\n",
        "\n",
        "#### Пример тематического моделирования с NLTK\n",
        "\n",
        "```python\n",
        "from nltk.corpus import reuters\n",
        "from gensim import corpora, models\n",
        "\n",
        "# Подготовка данных\n",
        "documents = [list(reuters.words(fileid)) for fileid in reuters.fileids()]\n",
        "\n",
        "# Создание словаря\n",
        "dictionary = corpora.Dictionary(documents)\n",
        "\n",
        "# Создание мешка слов (Bag of Words)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
        "\n",
        "# Обучение модели LDA (Latent Dirichlet Allocation)\n",
        "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
        "\n",
        "# Вывод тем\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "```\n",
        "\n",
        "**Объяснение:** Мы используем корпус Reuters для построения модели LDA, которая выделяет 10 тем в текстах. Тематическое моделирование полезно для анализа больших объемов данных, таких как новости, статьи или форумы.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "NLTK — это мощная и гибкая библиотека, которая поддерживает широкий спектр задач в области обработки естественного языка (NLP). Мы рассмотрели множество дополнительных возможностей NLTK, таких как работа с корпусами, анализ коллокаций, классификация текста, синтаксический анализ, извлечение именованных сущностей и тематическое моделирование.\n"
      ],
      "metadata": {
        "id": "MNa6YCPjeOdY"
      }
    }
  ]
}