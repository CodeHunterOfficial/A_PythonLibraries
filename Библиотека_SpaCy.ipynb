{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCdoUuzoYy8jAraVV/zVw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/A_PythonLibraries/blob/main/%D0%91%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Библиотека SpaCy\n",
        "\n",
        "## Введение\n",
        "\n",
        "SpaCy — это современная библиотека для обработки естественного языка (Natural Language Processing, NLP), которая сочетает высокую производительность с простотой использования. Она предназначена для профессионального использования в приложениях, связанных с извлечением информации, машинным обучением и анализом больших объемов текстовых данных. В отличие от более старых библиотек, таких как NLTK, SpaCy предоставляет готовые к использованию модели и инструменты для токенизации, синтаксического анализа, частеречной разметки, лемматизации, распознавания именованных сущностей (NER) и других задач.\n",
        "\n",
        "### Установка SpaCy\n",
        "\n",
        "Чтобы начать работу с SpaCy, необходимо установить библиотеку и загрузить языковую модель. Установка библиотеки осуществляется через пакетный менеджер `pip`:\n",
        "\n",
        "```bash\n",
        "pip install spacy\n",
        "```\n",
        "\n",
        "После этого требуется загрузить модель для выбранного языка. Например, для работы с английским языком можно использовать команду:\n",
        "\n",
        "```bash\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "### Языковые модели в SpaCy\n",
        "\n",
        "Языковые модели в SpaCy содержат различные компоненты для обработки текста. Каждая модель включает в себя:\n",
        "- **Токенизатор**: Разбивает текст на отдельные слова и знаки препинания.\n",
        "- **Частеречную разметку** (POS tagging): Определяет часть речи для каждого токена.\n",
        "- **Лемматизатор**: Приводит слова к их исходной форме.\n",
        "- **Синтаксический анализатор**: Строит дерево зависимостей предложений.\n",
        "- **Модуль распознавания именованных сущностей (NER)**: Определяет имена людей, места, даты и другие сущности.\n",
        "\n",
        "Теперь давайте рассмотрим основные возможности SpaCy на практике.\n",
        "\n",
        "## 1. Токенизация\n",
        "\n",
        "**Токенизация** — это процесс, при котором текст разбивается на отдельные слова, знаки препинания и другие значимые элементы, называемые токенами. В SpaCy токенизация выполняется автоматически при обработке текста с помощью загруженной языковой модели.\n",
        "\n",
        "### Пример токенизации текста:\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Загрузка английской языковой модели\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Пример текста\n",
        "text = \"Hello, world! Welcome to the world of SpaCy.\"\n",
        "\n",
        "# Обработка текста с помощью модели\n",
        "doc = nlp(text)\n",
        "\n",
        "# Вывод всех токенов\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "```\n",
        "\n",
        "### Объяснение:\n",
        "- Мы загружаем модель английского языка с помощью функции `spacy.load()`.\n",
        "- Передаем текст в модель, которая создает объект `doc`, представляющий собой обработанный текст.\n",
        "- Затем мы выводим каждый токен в тексте через цикл `for`, используя свойство `text` для каждого токена.\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Hello\n",
        ",\n",
        "world\n",
        "!\n",
        "Welcome\n",
        "to\n",
        "the\n",
        "world\n",
        "of\n",
        "SpaCy\n",
        ".\n",
        "```\n",
        "\n",
        "## 2. Лемматизация\n",
        "\n",
        "**Лемматизация** — это процесс приведения слов к их исходной форме, называемой **леммой**. Леммы позволяют нормализовать слова для их дальнейшего анализа, игнорируя грамматические изменения (например, спряжение глаголов).\n",
        "\n",
        "### Пример лемматизации:\n",
        "\n",
        "```python\n",
        "# Лемматизация токенов\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, Lemma: {token.lemma_}')\n",
        "```\n",
        "\n",
        "### Объяснение:\n",
        "- Мы выводим исходный текст каждого токена, а также его лемму с помощью свойства `lemma_`.\n",
        "- Лемматизация полезна при работе с текстами, чтобы сгруппировать разные формы одного и того же слова.\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Token: Hello, Lemma: hello\n",
        "Token: ,, Lemma: ,\n",
        "Token: world, Lemma: world\n",
        "Token: !, Lemma: !\n",
        "Token: Welcome, Lemma: welcome\n",
        "Token: to, Lemma: to\n",
        "Token: the, Lemma: the\n",
        "Token: world, Lemma: world\n",
        "Token: of, Lemma: of\n",
        "Token: SpaCy, Lemma: SpaCy\n",
        "Token: ., Lemma: .\n",
        "```\n",
        "\n",
        "## 3. Частеречная разметка (POS-tagging)\n",
        "\n",
        "**Частеречная разметка** (POS-tagging) — это процесс, при котором каждому слову в предложении присваивается его часть речи, например, существительное, глагол, прилагательное и т. д. Это позволяет лучше понять синтаксическую структуру текста и его грамматические зависимости.\n",
        "\n",
        "### Пример частеречной разметки:\n",
        "\n",
        "```python\n",
        "# Часть речи для каждого токена\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, POS: {token.pos_}, Detailed POS: {token.tag_}')\n",
        "```\n",
        "\n",
        "### Объяснение:\n",
        "- Мы выводим часть речи для каждого токена с помощью свойства `pos_` (например, NOUN — существительное, VERB — глагол).\n",
        "- Также выводим детализированную информацию о части речи с помощью `tag_`, которая даёт более точные грамматические характеристики.\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Token: Hello, POS: INTJ, Detailed POS: UH\n",
        "Token: ,, POS: PUNCT, Detailed POS: ,\n",
        "Token: world, POS: NOUN, Detailed POS: NN\n",
        "Token: !, POS: PUNCT, Detailed POS: .\n",
        "Token: Welcome, POS: VERB, Detailed POS: VB\n",
        "Token: to, POS: ADP, Detailed POS: IN\n",
        "Token: the, POS: DET, Detailed POS: DT\n",
        "Token: world, POS: NOUN, Detailed POS: NN\n",
        "Token: of, POS: ADP, Detailed POS: IN\n",
        "Token: SpaCy, POS: PROPN, Detailed POS: NNP\n",
        "Token: ., POS: PUNCT, Detailed POS: .\n",
        "```\n",
        "\n",
        "## 4. Синтаксический анализ (Dependency Parsing)\n",
        "\n",
        "**Синтаксический анализ** позволяет определить грамматические отношения между словами в предложении. В процессе синтаксического анализа строится дерево зависимостей, где каждое слово имеет связь с другими словами.\n",
        "\n",
        "### Пример синтаксического анализа:\n",
        "\n",
        "```python\n",
        "# Синтаксические зависимости и связи между токенами\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, Head: {token.head.text}, Dependency: {token.dep_}')\n",
        "```\n",
        "\n",
        "### Объяснение:\n",
        "- Свойство `head` показывает \"главное\" слово, к которому относится данный токен.\n",
        "- Свойство `dep_` описывает тип грамматической зависимости между токенами (например, `nsubj` — подлежащее, `obj` — дополнение).\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Token: Hello, Head: Welcome, Dependency: intj\n",
        "Token: ,, Head: Hello, Dependency: punct\n",
        "Token: world, Head: Welcome, Dependency: nsubj\n",
        "Token: !, Head: Hello, Dependency: punct\n",
        "Token: Welcome, Head: Welcome, Dependency: ROOT\n",
        "Token: to, Head: Welcome, Dependency: prep\n",
        "Token: the, Head: world, Dependency: det\n",
        "Token: world, Head: to, Dependency: pobj\n",
        "Token: of, Head: world, Dependency: prep\n",
        "Token: SpaCy, Head: of, Dependency: pobj\n",
        "Token: ., Head: Welcome, Dependency: punct\n",
        "```\n",
        "\n",
        "## 5. Распознавание именованных сущностей (NER)\n",
        "\n",
        "**Распознавание именованных сущностей (NER)** — это процесс идентификации и классификации ключевых сущностей в тексте, таких как имена людей, компании, города, даты и суммы денег. SpaCy поддерживает распознавание таких сущностей и присваивает им метки.\n",
        "\n",
        "### Пример распознавания именованных сущностей:\n",
        "\n",
        "```python\n",
        "# Распознавание именованных сущностей\n",
        "for ent in doc.ents:\n",
        "    print(f'Entity: {ent.text}, Label: {ent.label_}')\n",
        "```\n",
        "\n",
        "### Объяснение:\n",
        "- Мы используем атрибут `ents`, который хранит список всех распознанных сущностей в документе.\n",
        "- Свойство `label_` показывает тип сущности (например, `PERSON` — человек, `ORG` — организация, `DATE` — дата).\n",
        "\n",
        "**Результат (если есть именованные сущности в тексте):**\n",
        "```\n",
        "Entity: SpaCy, Label: ORG\n",
        "```\n",
        "\n",
        "В нашем примере нет явных именованных сущностей, кроме \"SpaCy\", которая распознана как организация (`ORG`).\n",
        "\n",
        "## 6. Работа с предложениями\n",
        "\n",
        "SpaCy также умеет разбивать текст на предложения. Это полезно для анализа текста на уровне предложений, например, при генерации текста или выделении отдельных предложений из абзацев.\n",
        "\n",
        "### Пример разбивки на предложения:\n",
        "\n",
        "```python\n",
        "# Разбивка текста на предложения\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n",
        "```\n",
        "\n",
        "### Объяснение:\n",
        "- Атрибут `sents` объекта `doc` содержит список всех предложений в тексте.\n",
        "- Мы проходим по каждому предложению и выводим его.\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Hello, world!\n",
        "Welcome to the world of SpaCy.\n",
        "```\n",
        "\n",
        "## 7. Работа с несколькими языками\n",
        "\n",
        "SpaCy поддерживает множество языков, включая русский, испанский, немецкий и другие. Для каждого языка доступны\n",
        "\n",
        " предобученные модели.\n",
        "\n",
        "### Пример работы с русским языком:\n",
        "\n",
        "Сначала нужно загрузить русскую модель:\n",
        "\n",
        "```bash\n",
        "python -m spacy download ru_core_news_sm\n",
        "```\n",
        "\n",
        "После загрузки модели можно использовать её для обработки текстов на русском языке.\n",
        "\n",
        "```python\n",
        "# Загрузка русской модели\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Пример русского текста\n",
        "doc = nlp(\"Привет, мир! Добро пожаловать в мир SpaCy.\")\n",
        "\n",
        "# Вывод токенов и частей речи\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, POS: {token.pos_}')\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Token: Привет, POS: INTJ\n",
        "Token: ,, POS: PUNCT\n",
        "Token: мир, POS: NOUN\n",
        "Token: !, POS: PUNCT\n",
        "Token: Добро, POS: NOUN\n",
        "Token: пожаловать, POS: VERB\n",
        "Token: в, POS: ADP\n",
        "Token: мир, POS: NOUN\n",
        "Token: SpaCy, POS: PROPN\n",
        "Token: ., POS: PUNCT\n",
        "```\n",
        "\n",
        "## Углубленное изучение возможностей библиотеки SpaCy\n",
        "\n",
        "### 1. Кастомизация пайплайна SpaCy\n",
        "\n",
        "Кастомизация пайплайна в SpaCy позволяет адаптировать стандартные компоненты обработки текста под специфические требования вашего проекта. Вы можете добавлять новые компоненты, изменять порядок существующих или полностью заменять их.\n",
        "\n",
        "#### Пример кастомизации пайплайна\n",
        "\n",
        "Предположим, что у вас есть текст, и вы хотите добавить свой компонент, который будет выделять определенные ключевые слова:\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Загрузка модели\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Кастомный компонент для выделения ключевых слов\n",
        "@Language.component(\"keyword_extractor\")\n",
        "def keyword_extractor(doc):\n",
        "    keywords = []\n",
        "    for token in doc:\n",
        "        if token.text.lower() in [\"spaCy\", \"nlp\", \"machine learning\"]:\n",
        "            keywords.append(token.text)\n",
        "    doc._.keywords = keywords\n",
        "    return doc\n",
        "\n",
        "# Регистрация компонента в пайплайне\n",
        "nlp.add_pipe(\"keyword_extractor\", last=True)\n",
        "\n",
        "# Обработка текста\n",
        "doc = nlp(\"SpaCy is an amazing library for NLP and machine learning.\")\n",
        "print(doc._.keywords)  # Вывод ключевых слов\n",
        "```\n",
        "\n",
        "#### Объяснение:\n",
        "- Мы создаем новый компонент `keyword_extractor`, который добавляет в документ список ключевых слов.\n",
        "- Ключевые слова добавляются в атрибут `._.keywords` документа.\n",
        "- Компонент регистрируется в пайплайне с помощью `add_pipe()`, а затем используется для обработки текста.\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "['SpaCy', 'machine learning']\n",
        "```\n",
        "\n",
        "### 2. Обучение кастомных моделей NER\n",
        "\n",
        "Создание кастомных моделей NER позволяет распознавать специфические сущности, которые могут быть не представлены в стандартных моделях SpaCy. Например, для медицинских текстов это могут быть названия лекарств.\n",
        "\n",
        "#### Пример обучения модели NER\n",
        "\n",
        "1. **Подготовка данных**: Данные должны быть размечены в формате, который SpaCy принимает для обучения моделей. Обычно это формат JSON.\n",
        "\n",
        "```python\n",
        "TRAIN_DATA = [\n",
        "    (\"Aspirin is a pain reliever.\", {\"entities\": [(0, 7, \"MEDICINE\")]}),\n",
        "    (\"Tylenol is also known as acetaminophen.\", {\"entities\": [(0, 7, \"MEDICINE\"), (30, 43, \"MEDICINE\")]}),\n",
        "]\n",
        "```\n",
        "\n",
        "2. **Обучение модели**:\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "\n",
        "# Загрузка базовой модели\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Добавление новой метки\n",
        "ner.add_label(\"MEDICINE\")\n",
        "\n",
        "# Обучение модели\n",
        "optimizer = nlp.begin_training()\n",
        "for epoch in range(30):\n",
        "    for text, annotations in TRAIN_DATA:\n",
        "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "        nlp.update([example], drop=0.5, losses={})\n",
        "```\n",
        "\n",
        "#### Объяснение:\n",
        "- Мы создаем тренировочные данные с размеченными сущностями и добавляем новую метку \"MEDICINE\".\n",
        "- Обучение происходит через 30 эпох, где каждый текст обновляет модель.\n",
        "\n",
        "### 3. Обработка текстов на разных языках\n",
        "\n",
        "SpaCy поддерживает множество языков, и каждая языковая модель имеет свои особенности. Чтобы использовать SpaCy для обработки текстов на другом языке, просто загрузите соответствующую языковую модель.\n",
        "\n",
        "#### Пример работы с русским языком:\n",
        "\n",
        "```bash\n",
        "python -m spacy download ru_core_news_sm\n",
        "```\n",
        "\n",
        "Затем в коде:\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Загрузка русской модели\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Пример текста на русском языке\n",
        "doc = nlp(\"Привет, мир! Добро пожаловать в мир SpaCy.\")\n",
        "\n",
        "# Вывод токенов и частей речи\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, POS: {token.pos_}, Lemma: {token.lemma_}')\n",
        "```\n",
        "\n",
        "### 4. Использование SpaCy с библиотеками для машинного обучения\n",
        "\n",
        "SpaCy может быть использован в сочетании с популярными библиотеками для машинного обучения, такими как Scikit-learn. Это позволяет подготавливать текстовые данные для обучения классификаторов и других моделей.\n",
        "\n",
        "#### Пример использования SpaCy с Scikit-learn:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Данные для классификации\n",
        "data = [\"I love SpaCy!\", \"SpaCy is amazing.\", \"I hate bugs.\", \"Bugs are annoying.\"]\n",
        "labels = [\"positive\", \"positive\", \"negative\", \"negative\"]\n",
        "\n",
        "# Создание модели\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(data, labels)\n",
        "\n",
        "# Прогнозирование\n",
        "predicted = model.predict([\"I enjoy working with SpaCy.\"])\n",
        "print(predicted)  # Вывод: ['positive']\n",
        "```\n",
        "\n",
        "#### Объяснение:\n",
        "- Мы используем `CountVectorizer` для преобразования текстов в числовые векторы.\n",
        "- Затем применяется `MultinomialNB` для классификации.\n",
        "\n",
        "### 5. Тональность текста и анализ сентимента\n",
        "\n",
        "Хотя SpaCy не имеет встроенных моделей анализа сентимента, вы можете использовать её для предобработки текста перед применением других библиотек.\n",
        "\n",
        "#### Пример анализа сентимента с TextBlob:\n",
        "\n",
        "```python\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Предобработка текста с помощью SpaCy\n",
        "doc = nlp(\"SpaCy is a great library for NLP!\")\n",
        "\n",
        "# Применение TextBlob для анализа сентимента\n",
        "blob = TextBlob(doc.text)\n",
        "print(blob.sentiment)  # Вывод: Sentiment(polarity=0.5, subjectivity=0.6)\n",
        "```\n",
        "\n",
        "### 6. Кастомные токенизаторы и правила токенизации\n",
        "\n",
        "Кастомные токенизаторы могут быть полезны, если стандартные правила токенизации не подходят для вашего текста. Например, вы можете создать токенизатор, который будет учитывать специфические знаки препинания или технические термины.\n",
        "\n",
        "#### Пример создания кастомного токенизатора:\n",
        "\n",
        "```python\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "# Создание кастомного токенизатора\n",
        "infixes = [r'(?<=[a-z])(?=[A-Z])']  # Пример для разбиения camelCase\n",
        "infix_regex = compile_infix_regex(infixes)\n",
        "\n",
        "tokenizer = Tokenizer(nlp.vocab, infix_finditer=infix_regex.finditer)\n",
        "\n",
        "doc = tokenizer(\"ThisIsAnExample\")\n",
        "print([token.text for token in doc])  # ['This', 'Is', 'An', 'Example']\n",
        "```\n",
        "\n",
        "### 7. Семантическая обработка и сходство текстов\n",
        "\n",
        "SpaCy может вычислять семантическое сходство между словами и предложениями с помощью word embeddings.\n",
        "\n",
        "#### Пример вычисления сходства:\n",
        "\n",
        "```python\n",
        "# Сравнение сходства между токенами\n",
        "doc1 = nlp(\"I love programming.\")\n",
        "doc2 = nlp(\"I enjoy coding.\")\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(f'Similarity: {similarity}')  # Выводит значение от 0 до 1\n",
        "```\n",
        "\n",
        "### 8. Работа с большими объёмами данных\n",
        "\n",
        "При работе с большими наборами данных важно оптимизировать использование памяти и производительность.\n",
        "\n",
        "#### Пример пакетной обработки:\n",
        "\n",
        "```python\n",
        "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]  # Массив больших данных\n",
        "docs = list(nlp.pipe(texts))  # Используем pipe для пакетной обработки\n",
        "```\n",
        "\n",
        "### 9. Интеграция с Gensim для анализа тем\n",
        "\n",
        "SpaCy может быть интегрирован с Gensim для выполнения тематического моделирования.\n",
        "\n",
        "#### Пример анализа тем с Gensim:\n",
        "\n",
        "```python\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Подготовка данных\n",
        "texts = [[\"spaCy\", \"is\", \"great\"], [\"nlp\", \"with\", \"python\"], [\"machine\", \"learning\"]]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Модель LDA\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
        "```\n",
        "\n",
        "### 10. Обработка структуры текста\n",
        "\n",
        "SpaCy может быть настроен для обработки структур текста, таких как абзацы и заголовки.\n",
        "\n",
        "#### Пример обработки структуры текста:\n",
        "\n",
        "```python\n",
        "# Разделение текста на абзацы\n",
        "text = \"This is the first paragraph.\\n\\nThis is the second paragraph.\"\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "for paragraph in paragraphs:\n",
        "    doc = nlp(paragraph)\n",
        "    print(f\"Paragraph: {paragraph}\")\n",
        "```\n",
        "\n",
        "### 11. Кастомные модели на основе правила\n",
        "\n",
        "Кастомные модели на основе правил могут быть использованы для решения простых задач, где не требуется машинное обучение.\n",
        "\n",
        "#### Пример создания модели на основе правил:\n",
        "\n",
        "```python\n",
        "# Простой пример нахождения чисел в тексте\n",
        "text = \"The price is 100 dollars and 50 cents.\"\n",
        "doc = nlp(text)\n",
        "numbers = [token.text for token\n",
        "\n",
        " in doc if token.like_num]\n",
        "print(numbers)  # ['100', '50']\n",
        "```\n",
        "\n",
        "### 12. Морфологический анализ\n",
        "\n",
        "SpaCy может выполнять морфологический анализ, что полезно для языков с богатой морфологией, таких как русский.\n",
        "\n",
        "#### Пример морфологического анализа:\n",
        "\n",
        "```python\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "doc = nlp(\"Мальчик играет в футбол.\")\n",
        "for token in doc:\n",
        "    print(f'Слово: {token.text}, Падеж: {token.morph.get(\"Case\")}, Число: {token.morph.get(\"Number\")}')\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Библиотека SpaCy предлагает богатый функционал для обработки естественного языка. От кастомизации пайплайнов до интеграции с другими библиотеками и анализом семантики — возможности SpaCy практически безграничны. Каждая из рассмотренных тем открывает новые горизонты для обработки текстовых данных, что делает SpaCy мощным инструментом для разработчиков и исследователей в области NLP.\n",
        "\n"
      ],
      "metadata": {
        "id": "mj9oE47q3nBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Вопросы для самопроверки\n",
        "\n",
        "\n",
        "1. Что такое SpaCy и в чем его основные преимущества по сравнению с другими библиотеками для обработки естественного языка?\n",
        "\n",
        "2. Как установить SpaCy и загрузить языковую модель?\n",
        "\n",
        "3. Что такое токенизация и как она реализована в SpaCy?\n",
        "\n",
        "4. Как выполнить лемматизацию текста с помощью SpaCy?\n",
        "\n",
        "5. В чем разница между частеречной разметкой (POS-tagging) и синтаксическим анализом?\n",
        "\n",
        "6. Как работает распознавание именованных сущностей (NER) в SpaCy и какие типы сущностей можно распознавать?\n",
        "\n",
        "7. Как разбить текст на предложения с помощью SpaCy?\n",
        "\n",
        "8. Как можно кастомизировать пайплайн SpaCy для добавления новых компонентов?\n",
        "\n",
        "9. Как обучить кастомную модель NER на основе размеченных данных?\n",
        "\n",
        "10. Как использовать `nlp.pipe()` для пакетной обработки текстов и какие преимущества это дает?\n",
        "\n",
        "11. Как можно извлечь числовые значения из текста с помощью SpaCy?\n",
        "\n",
        "12. В чем заключается морфологический анализ и как его можно выполнить в SpaCy?\n",
        "\n",
        "13. Как использовать SpaCy для анализа сентимента текста?\n",
        "\n",
        "14. Как реализовать кастомный токенизатор для специфических текстов?\n",
        "\n",
        "15. Как вычислить семантическое сходство между двумя текстами с помощью SpaCy?\n",
        "\n",
        "16. Как можно использовать SpaCy в сочетании с библиотеками машинного обучения, такими как Scikit-learn?\n",
        "\n",
        "17. Как визуализировать деревья зависимостей с помощью `displacy`?\n",
        "\n",
        "18. Какие существуют способы извлечения информации из текстов с помощью SpaCy?\n",
        "\n",
        "19. Как обрабатывать тексты на нескольких языках с использованием SpaCy?\n",
        "\n",
        "20. Какие методы оптимизации производительности можно использовать при работе с большими объемами данных в SpaCy?\n"
      ],
      "metadata": {
        "id": "q1vDRIqT5GBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задачи для самостоятельной работы\n",
        "\n",
        "1. Установите SpaCy и загрузите английскую языковую модель. Проверьте, что установка прошла успешно, обработав простой текст.\n",
        "\n",
        "2. Реализуйте токенизацию текста на английском языке и выведите все токены в виде списка.\n",
        "\n",
        "3. Напишите код для лемматизации набора предложений и выведите исходные слова и их леммы.\n",
        "\n",
        "4. Выполните частеречную разметку (POS-tagging) для текста и выведите каждое слово с его частью речи.\n",
        "\n",
        "5. Создайте синтаксический анализатор для предложения и выведите дерево зависимостей.\n",
        "\n",
        "6. Реализуйте распознавание именованных сущностей (NER) в тексте и выведите найденные сущности с их метками.\n",
        "\n",
        "7. Разбейте текст на предложения и выведите их по одному.\n",
        "\n",
        "8. Загрузите русскую языковую модель и выполните токенизацию русского текста. Выведите токены и их части речи.\n",
        "\n",
        "9. Создайте кастомный компонент для выделения ключевых слов в тексте и добавьте его в пайплайн SpaCy.\n",
        "\n",
        "10. Обучите простую модель NER на основе размеченных данных, используя SpaCy, и протестируйте ее на новом тексте.\n",
        "\n",
        "11. Используйте `nlp.pipe()` для пакетной обработки большого объема текстов и измерьте время выполнения.\n",
        "\n",
        "12. Реализуйте анализ сентимента текста с использованием SpaCy и другой библиотеки, например, TextBlob.\n",
        "\n",
        "13. Используйте SpaCy для извлечения чисел из текста и выведите их в виде списка.\n",
        "\n",
        "14. Создайте кастомный токенизатор для обработки специфических текстов (например, для медицинских терминов).\n",
        "\n",
        "15. Проведите семантический анализ сходства между двумя предложениями с помощью SpaCy.\n",
        "\n",
        "16. Реализуйте анализ частоты слов в тексте и выведите 10 самых частых слов.\n",
        "\n",
        "17. Используйте SpaCy для обработки текстов на нескольких языках и выведите результаты для каждого языка.\n",
        "\n",
        "18. Создайте модель для классификации текста, используя SpaCy для предобработки и Scikit-learn для обучения.\n",
        "\n",
        "19. Реализуйте морфологический анализ для русского текста и выведите падежи и числа слов.\n",
        "\n",
        "20. Используйте SpaCy для извлечения всех имен собственных из текста и выведите их в формате JSON.\n",
        "\n",
        "21. Создайте кастомные правила для выделения определенных паттернов в тексте (например, даты или адреса).\n",
        "\n",
        "22. Реализуйте анализ структуры текста, разбивая его на абзацы и предложения, и выведите результаты.\n",
        "\n",
        "23. Используйте SpaCy для визуализации деревьев зависимостей с помощью библиотеки `displacy`.\n",
        "\n",
        "24. Обучите кастомную модель для распознавания специфических сущностей в текстах (например, названия лекарств).\n",
        "\n",
        "25. Реализуйте анализ текста на предмет наличия негативных и позитивных слов с помощью SpaCy.\n",
        "\n",
        "26. Используйте SpaCy для создания системы рекомендаций на основе анализа текстов.\n",
        "\n",
        "27. Проведите анализ тональности текстов с использованием SpaCy и визуализируйте результаты.\n",
        "\n",
        "28. Реализуйте автоматизированный процесс извлечения информации из текстов (например, названия компаний, даты).\n",
        "\n",
        "29. Создайте приложение, которое будет обрабатывать вводимый текст и выводить результаты анализа (токены, части речи, сущности).\n",
        "\n",
        "30. Исследуйте возможности SpaCy для работы с большими объемами данных и оптимизируйте производительность обработки текста."
      ],
      "metadata": {
        "id": "QVjp2VGO5CT-"
      }
    }
  ]
}